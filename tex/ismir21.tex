\newcommand{\imgb}{imgs/ismir21}

With the advent of reasonably large labeled datasets such as the VGMIDI dataset \cite{ferreira_2019}, it became feasible to investigate affective music generation \cite{williams2015investigating} as a problem of steering the probability distribution of language models (LMs) towards a given emotion. In this framing, the problem consists of training both a LM and a music emotion classifier, then using the probability distribution of the former to steer the probability distribution of the latter.

In this paper, we propose a new approach to this problem with Monte Carlo Tree Search (MCTS). At decoding time, we run multiple search iterations to build a distribution of node visits that steers
the LM distribution towards a target emotion. During the search, we compute the value of each node using a music emotion classifier and use Predictor Upper Confidence for Trees (PUCT) to decide the order of node expansions. We sample from this new distribution to decide the next token in the sequence.

We search over the space learned by a Music Transformer LM \cite {huang2018music} with the unlabelled pieces of the VGMIDI dataset. In this paper, we extended the number of unlabelled pieces of the VGMIDI dataset from 728 to 3640. Following the approach of \cite{ferreira_2019} and \cite{ferreira2020computer}, we train the music emotion classifier with 200 labeled pieces of the VGMIDI dataset by fine-tuning the Music Transformer LM with a classification head. The VGMIDI dataset labels data according to a valence-arousal model of emotion \cite{russell1980circumplex}. All previous works have considered the emotion classification problem as two independent binary problems: one for valence classification and another for arousal classification. In this paper, we consider the emotion classification problem as a multiclass problem, where we map each of the four quadrants of the valence-arousal model to a label.

We evaluate our MCTS method with two listening tests (human evaluations), one to measure the quality of the generated pieces and one to measure the accuracy of our method in generating pieces with a given emotion. In the first test, we ask for the preferences of the listeners among our method, the validation data (human compositions), TopK sampling and Stochastic Bi-Objective Beam Search (SBBS) \cite{ferreira2020computer}. In the second study, we ask listeners to annotate the emotion of pieces generated by our method, TopK sampling and SBBS. Results showed that our method outperforms SBBS in terms of music quality while keeping the same accuracy when conveying target emotions. We perform an expressivity analysis \cite{smith2010analyzing} to better understand how MCTS is conveying each target emotion. The frequencies of pitch classes and note
durations suggest that MCTS can reproduce some common composition practices used by human composers.

To the best of our knowledge, this is the first work to apply MCTS as an algorithm to decode language models. Moreover, this is the first time MCTS is being used to control emotions in generated symbolic music. Given that MCTS is agnostic to the classifier used to steer the distribution of the language model, we believe our method can be used to control different features of music that can be discriminated by a classifier. We also believe that MCTS can be generalized to decode and control language models in other sequential domains such as text.

\section{Language and Emotion Models}

In this section we describe the language and emotion models used to guide the generation of music with perceived emotion.

\subsection{Language Model}

As language model we use a Music Transformer \cite{huang2018music}, which we trained on the unlabelled pieces of the VGMIDI dataset. Originally, the VGMIDI dataset had 728 unlabelled pieces, but we expand it to 3,640 pieces in this work. The new pieces are piano arrangements of video game music create by the NinSheetMusic community\footnote{\url{https://www.ninsheetmusic.org/}}. We used
a Music Transformer because it is currently one of the state-of-the-art methods for symbolic music generation~\cite{huang2018music}. We used the VGMIDI dataset, as opposed to
MAESTRO \cite{hawthorne2018enabling} or other large dataset of symbolic music, to be able to train both the language and emotion models on similar datasets.

We encoded VGMIDI pieces using a different vocabulary than the original one proposed with the Music Transformer \cite{huang2018music}. Aiming at reducing the length of the music sequences, we map MIDI files to sequences using a large expressive vocabulary
instead of a compact one. To create a sequence from a MIDI file, we
% assume the piece tempo is 120 bpm and
discretize the starting times of all the notes into a sequence of time-steps. We then process each time-step in order, generating a token $n_{p, d, v}$ for each note in the time-step.

The three parameters of a note token $n_{p, d, v}$ are pitch $p$, duration $d$ and velocity $v$, respectively. In order to constrain the possible combinations of note tokens, we limit the pitch values to $30 \leq p \leq 96$. Duration $d$ is limited by the types: breve, whole, half, quarter, eighth, 16th and 32nd. We also
consider the dotted versions of these types (maximum of 3 dots).
Velocities are limited to the values $v \in [32, 36, 40, \cdots, 128]$.
After processing each time step, we generate a token $r_d$ that represents a rest with a given duration. At the end of all time-steps, we include a token ``$.$'' (period) that represents the end of the piece.

This encoding scheme yields a vocabulary with 44,346 tokens. This is
orders of magnitude larger than most of the other vocabularies in the literature \cite{briot2017deep}. By having more tokens, we have more options to choose from at any given point of the generative process, which increases the search complexity. On the other hand, we considerably reduce the length of the encoded pieces, which reduces the search complexity. We mitigate the effects of having more options to choose from by only considering the top $k$ tokens according to the language model at any decision point of the generative process.
%
%This allows the generative search procedure to spend
%fewer simulations to produce a complete piece.  %to explore a complete piece.
Moreover,
according to \cite{holtzman2018learning}, language models tend to generate less
repetitive sequences with larger vocabularies.

We denote the probability of a sequence of tokens $s$ according to the language model as $L(s)$ and the probability of the sequence $s$ added of the token $l$ as $L(s, l)$.

\subsection{Emotion Model}\label{sec:emotion_classifier}

Ferreira et al. \cite{ferreira_2019, ferreira2020computer} showed that
fine-tuning a language model with an extra classification head yields a
better model than training a classifier from scratch with the
same architecture of the language model. We follow a similar approach
in this paper, by fine-tuning a Music Transformer instead of
an LSTM \cite{ferreira_2019} or a GPT-2 Transformer \cite{ferreira2020computer}.
We train this fine-tuned Music Transformer with the labelled pieces
of the VGMIDI dataset.

Typically, symbolic music emotion classification is approached as two
independent binary problems, one for valence and one for arousal \cite{ferreira_2019, ferreira2020computer}. In this paper, we define
it as a multiclass problem. We consider four different ``emotions'':
high valence and arousal (E0), low valence and high arousal (E1),
low valence and arousal (E2) and high valence and low arousal (E4).
Each labelled piece in the VGMIDI dataset has a valence label $v \in [-1, 1]$ and a arousal label $a \in [-1, 1]$. We map a pair of values to a categorical label (E0, E1, E2 or E3) by getting the quadrant in which $(v,a)$ lie in. Thus, a piece with values $(1,1)$ is mapped
to E0, $(-1,1)$ is mapped to E1, $(-1,-1)$ is mapped to E2
and $(1, -1)$ is mapped to E3. This mapping yielded 76 pieces with label E0, 38 with label E1, 27 with label E2 and 59 with E3.
We use this multiclass approach to simplify the search task of controlling emotion of the generated pieces. Instead of having
the combination of two models as an emotion score, we only have one.

Since our search evaluates the emotion of sequences of varied length (see the detail in Section~\ref{sec:puct}),
%a node with the emotion classifier during every expansion,
we train our emotion model with
prefixes of different length extracted from the labelled pieces. Thus, during training, the classifier
learns a mapping of a sequence to an emotion considering different lengths. %This
%step is specially helpful to guide the search in the beginning of the %piece.  %, that returns a number between 0 and 1 for each of the four

We denote our emotion model by $E$ and write $E(s, e)$ to denote the probability of a sequence $s$ being perceived as a piece of emotion $e$.

\section{PUCT for Music Generation}
\label{sec:puct}

% MCTS
Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm traditionally used to
play board games with large search spaces. Before taking
an action at the current game state, MCTS iteratively explores
the branches of the search tree, looking ahead to determine how different game moves could lead to stronger or weaker positions of the game. MCTS variations use a variety of algorithms for deciding which branches of the tree to explore next. For example, UCT~\cite{Kocsis2006} uses the UCB1 formula for deciding which branches to expand in each iteration of the algorithm, while AlphaZero  uses the PUCT formula~\cite{rosin2011pucb}. Our approach, which we explain in this section, also employs PUCT to generate music with a specific perceived emotion.

PUCT receives a prefix $s = [x_0, x_1, \cdots, x_t]$ of an existing musical piece as input, which will bias the process; an emotion model $E$; a language model $L$; a set of $k$ symbols that is used to focus the search on the symbols with higher probability according to $L$; a search budget $d$ that defines the amount of search PUCT can perform before adding the next symbol to the musical sequence; and a target perceived emotion $e$. PUCT returns a sequence $s'$ with prefix $s$ for which $E(s', e)$ and $L(s')$ are maximized. %and that maximizes the probability of the sequence according to the language model $L$.

PUCT grows a search tree where each node $n$ in the tree represents a sequence of symbols from our vocabulary. The root of the tree represents the prefix $x$ provided as input. Each edge $(n, n')$ from node $n$ to node $n'$ represents the addition of a symbol to the sequence $n$ ($n'$ is one symbol larger than $n$). We say that $n'$ is a child of $n$. Since each node $n$ represents a sequence $s$, we use $n$ and $s$ interchangeably.  % of the tree represents the addition of a note from the vocabulary
Initially, PUCT's tree is of size one as it only contains the root of the tree. In each iteration, PUCT performs the following steps to add a new node to the tree: (1) selection, (2) expansion, (3) simulation and (4) backpropagation. We explain each of these steps and their intuition.

\vspace{0.1in}
\noindent
\textbf{Selection:} For each node $n$ in the PUCT we select the symbol $l$ that maximizes the following equation.
\begin{equation}
\argmax_{l} Q(n, l) + c \times L(n, l) \times \frac{\sqrt{N(n)}}{1 + N(n, l)} \,,
\label{eq:puct}
\end{equation}
\noindent
where $Q(n, l)$ is a value computed based on the emotion model (explained in the simulation and backpropagation steps), $c$ is an exploration constant, $L(n, l)$ is the language model's probability of adding symbol $l$ to the sequence $n$, $N(n)$ is the number of times node $n$ was visited in the selection step, and $N(n, l)$ is the number of times symbol $l$ was chosen at node $n$ in a selection step. Our implementation considers only the $k$ symbols with largest probable value according to the language model in the $\argmax$ operator shown above. This is because the vocabulary used is too large and we allow the search to focus on sequences that are more promising according to the language model. PUCT recursively selects nodes in the tree according to Equation~\ref{eq:puct} until the symbol $l$ at a node $n$ leads to a sequence whose node $n'$ is not in the PUCT tree.

\vspace{0.1in}
\noindent
\textbf{Expansion:} The node $n'$ returned in the selection step is then added to the PUCT tree and its statistics are initialized: $N(n') = 1$, $N(n', l) = 0$ and $Q(n', l) = 0$ for all top $k$ symbols $l$ according to the probability $L(n', l)$.

\vspace{0.1in}
\noindent
\textbf{Simulation:} In this step we evaluate the sequence represented in the recently added node $n'$ according to the target emotion. The $Q(n, l)$-value (recall that adding $l$ to $n$ generated the node $n'$) is given by $E(n', e)$. The value of $N(n, l) = 1$ as this is the first time $l$ is selected in node $n$.
%Inspired in the scheme employed in AlphaZero~\cite{alphazero},
%instead of adding symbols to the sequence node $n'$ represents until reaching an end-of-music symbol,
%we use $E(n', e)$ as the $Q(n, l)$-value.

\vspace{0.1in}
\noindent
\textbf{Backpropagation:} The value of $Q(n, l)$ is used to update the $Q$-values of all the other node-symbol pairs selected in the first step of the algorithm. This is achieved by following the path in the tree from the selection step in reverse order and updating the statistics of each node $n$ and node-symbol pairs $(n, l)$ as follows.
\begin{align*}
Q(n, l) &= \frac{Q(n, l) \times N(n, l) + E(n', e)}{N(n, l) + 1} \\
N(n, l) &= N(n, l) + 1 \,.
\end{align*}
The $Q(n, l)$-values are the average $E$-values of sequences with prefix given by $n$. The value of $Q(n, l)$ estimates how continuations of the sequence $n$ added to the symbol $l$ is with respect to the target emotion. The backpropagation step completes an iteration of PUCT.

In the next iteration, PUCT will perform the four steps described above, but with updated values of $N$ and $Q$ for the node-symbol pairs selected in the previous iteration. Equation~\ref{eq:puct} guarantees that the sequences $n$ that maximize the value of $E(n, e)$ are visited more often as they will have larger values of $Q$. The PUCT formula also accounts for the probability given by the language model, giving preference to sequences with higher probability according to $L$. Finally, the term $\frac{\sqrt{N(n)}}{1 + N(n, l)}$ certifies that all nodes have a chance of being explored by the search.
%allows even nodes that are considered unpromising by the language and emotion models to be explored by the search.

PUCT performs $d$ iterations before deciding which symbol will be added to the sequence represented at the root of the tree. That is, the search budget of $d$ iterations is to decide the next symbol of the sequence. Let $n$ be the root of the tree. The symbol $l$ that will be added to the sequence $n$ is sampled from the distribution given by the values $\frac{N(n)}{\sum_{l} N(n, l)}$. The node $n'$ resulting from the addition of $l$ to $n$ becomes the new root of the tree and we perform another PUCT search with budget $d$ to choose the next symbol to be added to $n'$. This process is repeated until the next symbol added is the end-of-music symbol.

The PUCT search can be seen as an operator that changes the probability distribution over symbols given by the language model such that it accounts for the target emotion model. This is because the distribution given by $\frac{N(n)}{\sum_{l} N(n, l)}$ will favor symbols that lead to pieces matching the target emotion as nodes representing such pieces are visited more often during search.


%% Alpha Zero
%MCTS became recently very popular with the success of the AlphaZero algorithm \cite{}, which achieved better-than-human
%performance on the game of Go. In AlphaZero, MCTS is used to
%improve a neural network that predicts the probability of a game
%state lead to a win.
%
%We apply MCTS for Music Generation inspired by AlphaZero. We explore the
%search space to create a distribution of node visits and  we sample from this
%distribution every time we want to take an action. In the music generation context, each state of the search space is a prefix of the music piece we are generating. An action consists of adding a token to the current state. Potentially, all tokens in the vocabulary are valid actions from any state. However, we limit the set of possible actions by allowing only the the top k tokens with higher probability as given by the language model.
%
%As showed in Algorithm \ref{alg:mcts}, the initial state of the MCTS search is a given prior sequence x = $[x_0, x_1, ..., x_t]$ (line 1). We run a number $d$ of MCTS steps in order to update the distribution $N[s]$ of node visits (lines 4-5) and sample the token $x_{t+1}$ from the resulting distribution (line 6).
%
%\begin{algorithm}[h]
%\caption{MCTS For Music Generation}
%\label{alg:mcts}
%\begin{algorithmic}[1]
%\REQUIRE Prior sequence $x$, number of steps $d$, music emotion classifier $E_m$, language model $L$,  number of top $k$ symbols to consider in $L$
%\ENSURE A generated piece $x'$
%
%\STATE $s \gets x$
%\STATE visited $\gets \{\}$
%
%\WHILE{True}
%    \FOR{i in 0...$d$}
%        \STATE \texttt{MCTS}($s$)
%    \ENDFOR
%
%\STATE $s \gets$ sample($\frac{N[s]}{\sum_{c} N[s][c]}$)
%
%\ENDWHILE
%
%\end{algorithmic}
%\end{algorithm}
%
%Each MCTS step is formally described in Algorithm \ref{alg:mcts_step}. We first check if the
%current state $s$ is terminal (line 1). If this is true, we just return
%it's value $E_m(s)$ computed with the emotion classifier (line 2). Otherwise, we check
%if the state $s$ has already been visited (line 3). If it has not been visited, we
%add that state to the set of visited nodes and (a) compute the probability $P[s][c]$ of every next state $c$ of $s$ with the language model, (b) initialize the $Q[s][c]$ values with zero for
%every next state $c$ of $s$, (c) initialize the number $N[s][c]$ of visits with zero for every
%next state $c$ of $s$ and (d) return the value $E_m(s)$ of the state $s$ as given by the emotion classifier. Note that we never do simulations (or rollouts) to calculate the value of a state.
%Instead, we consult the emotion classifier to do so.
%
%\begin{algorithm}[h]
%\caption{MCTS Step}
%\label{alg:mcts_step}
%\begin{algorithmic}[1]
%\REQUIRE Current state $s$, music emotion classifier $E_m$, language model $L$,  number of top $k$ symbols to consider in $L$
%\ENSURE Value of leaves $l$
%
%\IF {$s$ is terminal}
%    \RETURN $E_m(s)$
%\ENDIF
%
%\IF {$s_t$ not in visited}
%    \STATE visited $\gets$ visited $\cup s_t$
%    \STATE $P[s] \gets L(s, k)$
%    \STATE $Q[s] \gets \{\}$
%    \STATE $N[s] \gets \{\}$
%    \RETURN $E_m(s)$
%\ENDIF
%
%\STATE $l \gets$ argmax($Q[s][l] + c * P[s][l] * \frac{\sqrt{\sum{N[s]}}}{1 + N[s][l]}$))
%\STATE $v \gets \texttt{MCTS}(l)$
%
%\IF {$l$ in $Q[s]$}
%    \STATE $Q[s][l] \gets (Q[s][l] * N[s][l] + v)/(N[s][l] + 1)$
%    \STATE $N[s][l] \gets N[s][l] + 1$
%\ELSE
%    \STATE $Q[s][l] \gets v$
%    \STATE $N[s][l] \gets 1$
%\ENDIF
%
%\RETURN $v$
%
%\end{algorithmic}
%\end{algorithm}

\section{Experiments}

We evaluate MCTS with two listening tests (user studies), one for measuring the quality of the generated pieces and one for measuring the accuracy in conveying a target emotion. All experiments were performed via Amazon Mechanical Turk (MTurk). For both experiments,
we compared MCTS against SBBS (see Section \ref{sec:related_work}), TopK sampling, and human composed pieces (from the validation data used with the emotion classifier).
MCTS is not compared against Ferreira and Whitehead \cite{ferreira_2019} because their approach is limited to sentiment.

For each model, we generated 10 pieces with 512 tokens for each target emotion $e \in [0, 1, 2, 3]$ (total of 160 pieces). Each model generated 40 pieces with the same set of 40 prime sequences (one prime per piece). Each prime sequence is 32 tokens long and is selected at random from the VGMIDI validation pieces with the target emotion. For the ``human'' model, pieces are ``generated'' by simply extracting the first 512 tokens of the piece with the given prime.

To generate the pieces, we first trained a Music Transformer LM with 4 layers (transformer blocks) and a maximum sequence length of 2048 tokens. We used 8 attention heads and an embedding layer with size 384. The size of the Feed-Forward layers in the each transformer block was set to 1024. This
LM was trained with the 3640 unlabelled pieces of the extended VGMIDI dataset. We use 3094 (85\%) of the pieces for training and 546 (15\%) for testing.
All unlabelled pieces were augmented by (a) transposing to every key, (b)
increasing and decreasing the tempo by 10\% and (c) increasing and decreasing
the velocity of all notes in by 10\% \cite{oore2017learning}.

We then trained the emotion classifier by fine-tuning the Music Transformer LM with an extra classification head on top. The emotion classifier was trained with the 200 labelled pieces of the VGMIDI dataset. We used 140 (70\%) pieces for training and 30 (30\%) for testing. After training, the losses of the Music Transformer LM are 0.54 (training set) and 0.73 (validation set). The validation accuracy of the emotion classifier is 61\%.

During generation time, we set the $k = 128$ to filter the language model distribution in MCTS,
SBBS and TopK. The number of beams for SBBS was set to $b = 4$. For MCTS, the number of simulation steps set was set to $30$ and the exploration weight to $16$.

% TODO: add prefixes sizes

\subsection{Quality}

In the quality listening test, we performed a pairwise comparison similar to the human evaluation in \cite {huang2018music}. Human subjects were presented with two generated pieces from two different models that were given the same priming sequence. The two pieces were
presented side-by-side and the human subjects were asked to select which one is more musical using a 5-point Likert scale. In this scale, 1 means "Left piece is much more musical", 2
means "Left piece is slightly more musical", 3 means "Tie", 4 means "Right piece is slightly more musical" and 5 means "Right piece is much more musical". The order of the two pieces was randomized to avoid ordering bias.

We assigned 3 MTurk workers for each of the 240 pairs of generated pieces. In order to reduce noise in the results (mainly caused by random choices in Amazon MTurk), we included a test evaluation for each human subject. This test is another pair of pieces to be evaluated with the same Likert scale, but one piece is a human composed piece and the other one is sampled from the Language Model without TopK filtering and temperature equal to 1.5 (forcing the sample to have poor quality). We also asked the subjects to briefly justify their choice with 1-3 short sentences. We filtered out participants who failed the test evaluation (choosing the sampled piece as more musical) or didn't write explanations longer than 5 words. In total, this experiment yielded 389 comparisons. Each pair was evaluated at least once.

Table \ref{tab:quality} shows the results of the quality test. The top part of the table shows the number of wins, ties and losses of one model against another.  MCTS performed exactly like TopK sampling and outperformed SBBS by ten wins. Surprisingly, MCTS won against human composed pieces 12 times and tied 9 times. SBBS performed worse than TopK sampling, winning 26 times and loosing 31. As expected, all models performed worse than human compositions. The bottom part of the table shows the percentage of wins, ties and losses for one model against all other models. Percentages are reported because, due the filtering of the participants, the amount of comparisons for each model is not the same. The aggregated results also show that
MCTS performs better than SBBS and the same as TopK sampling.
A Kruskal-Wallis H test of the subject choices (values from 1 to 5) showes that there is a statistically significant difference between the models with p = 1.5e-4 < 0.01.

% TODO: Run statistical tests
\begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
    \toprule
   \multicolumn{2}{c}{\textbf{One-Vs-One}} & \textbf{Wins} & \textbf{Ties} & \textbf{Losses} \\
    \midrule
    \textbf{MCTS} & \textbf{TopK } & 25 & 13 & 25 \\
    \textbf{MCTS} & \textbf{SBBS } & 34 & 8  & 24 \\
    \textbf{MCTS} & \textbf{Human} & 12 & 9  & 41 \\
    \textbf{TopK} & \textbf{SBBS } & 31 & 6  & 26 \\
    \textbf{TopK} & \textbf{Human} & 15 & 7  & 45 \\
    \textbf{SBBS} & \textbf{Human} & 15 & 8  & 45 \\
    \midrule
    \multicolumn{2}{c}{\textbf{One-Vs-Rest}} & \textbf{Wins \%} & \textbf{Ties \%} & \textbf{Losses \%}  \\
    \midrule
    \multicolumn{2}{c}{\textbf{MCTS}} & 38 & 15 & 47  \\
    \multicolumn{2}{c}{\textbf{SBBS}} & 33 & 12 & 55  \\
    \multicolumn{2}{c}{\textbf{TopK}} & 37 & 13 & 50 \\
    \multicolumn{2}{c}{\textbf{Human}} & 66 & 12 & 22 \\
    \bottomrule
    \end{tabular}
    \caption{Results of the quality listening test. The top part of the table reports the number of wins, ties and losses for a model against each other model. The results are stated with respect to the left model. For example, MCTS won against SBBS 34 times and lost to SBBS 24 times. The bottom part of the table shows the percentage of wins, ties and losses for a model against all the others.}
    \label{tab:quality}
\end{table}

% \begin{table}[h]
%     \centering
%     % \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{ccccc}
%     \toprule
%   \multicolumn{2}{c}{\textbf{Model A}} & \multicolumn{2}{c}{\textbf{Model B}} & \textbf{p} \\
%     \midrule
%     \textbf{MCTS} & (71,30,90) & \textbf{TopK } & (71,26,96) & 0 \\
%     \textbf{MCTS} & (71,30,90) & \textbf{SBBS } & (65,22,110) & 0 \\
%     \textbf{MCTS} & (71,30,90) & \textbf{Human} & (131,24,42) & 0\\
%     \textbf{TopK} & (71,26,96) & \textbf{SBBS } & (65,22,110) & 0 \\
%     \textbf{TopK} & (71,26,96) & \textbf{Human} & (131,24,42) & 0 \\
%     \textbf{SBBS} & (65,22,110) & \textbf{Human} & (131,24,42) & 0\\
%     \bottomrule
%     \end{tabular}
%     \caption{Accuracy in \% of both the GPT-2 and LSTM models for music emotion classification. }
%     \label{tab:sent_accuracy}
% \end{table}

\subsection{Emotion}

In the emotion listening test, we ask human subjects to annotate the generated pieces using the same tool designed to annotate the VGMIDI dataset. In this tool\footnote{\url{https://github.com/lucasnfe/adl-music-annotation}}, human subjects
% are presented with one piece at a time and
are asked to annotate them according to a valence-arousal model of emotion. An annotation result is a time-series of valence-arousal pairs where each element corresponds to a chunk (a bar if the piece has 4/4 time signature) of the piece. For this experiment, we assigned 3 MTurk workers for each piece generated by the MCTS, SBBS and TopK models (total of 360 annotations). We don't re-annotate the human pieces because they are the ground truth data used to train the emotion classification model that is base for both MCTS and SBBS. No annotations were filtered out in this experiment.

We measure the accuracy of a method in conveying a target emotion with the
percentage of chunks in the annotations that match the target emotion. We map each
valence-arousal pair to an emotion label by getting the quadrant of that valence-arousal pair
(see Section \ref{sec:emotion_classifier} for details).  Table \ref{tab:emotion} reports
the accuracy of each model for each emotion. Overall, MCTS outperformed TopK (no emotion control) by an average of 15\%. MCTS performed very similar to SBBS, with slightly better average accuracy. However, it is important to highlight that MCTS was able to considerably
improve the accuracy in conveying the two least represented emotions (E1 and E2) in the
VGMIDI dataset. This is an important results since labelling symbolic music according to emotion is a very expensive task.

% TODO: Add standard deviation
\begin{table}[h]
    \centering
    % \setlength{\tabcolsep}{4pt}
    \begin{tabular}{cccccc}
    \toprule
    \textbf{Model} & \textbf{E0} & \textbf{E1}  & \textbf{E2} & \textbf{E3} & \textbf{Avg.} \\
    \midrule
    \textbf{MCTS} & \textbf{72} & \textbf{52} & \textbf{37} & 57 & \textbf{54} \\
    \textbf{SBBS} & 67 & 41 & 30 & \textbf{70} & 52 \\
    \textbf{TopK} & 61 & 18 & 26 & 53 & 39 \\
    \bottomrule
    \end{tabular}
    \caption{Accuracy of each model in conveying the target emotions E0, E1, E2 and E3. }
    \label{tab:emotion}
\end{table}

TopK performed reasonably well on average (39\%), however, this was primarily due to its ability to convey the two most represented emotions in the training data (E0 and E3). These two emotions very likely are more represented in the unlabelled data as well, which was used to train the language model. Moreover, even though TopK sampling does not control emotion, the prime sequences they used to generate the pieces had the target emotions. Thus, TopK (like all other models) is conditioned with this prime sequence towards the desired emotion. However, because TopK does not consider emotion when generating pieces, eventually it starts sampling tokens that deviate from the target emotion.

Although MCTS performed similarly to SBBS in terms of emotion, MCTS outperformed SBBS considerably with respect to music quality. One reason for SBBS being as good as MCTS at conveying emotions is that SBBS tends to generate repetitive pieces, once that maximizes both the probabilities of the language model and the emotion model. Since SBBS doesn't do backtracking, when it generates a good pattern that is likely and that conveys the target emotion, it tends to repeat that pattern.

Figure \ref{fig:ex_pieces} illustrates this problem with examples of pieces generated by MCTS and SBBS. These two pieces were given the same prime sequence with the target emotion E2 (low valence and arousal). The MCTS piece develops the prime sequence exploring mainly the final part of the prime sequence. The generated continuation is formed by two different sections and each section is repeated once. The SBBS piece, on the other hand, simply repeats the final part of the prime sequence until the end of the piece. This is a case where SBBS repeated a given pattern that maximized the probability of the language model and the emotion model. This example also shows how backtracking allows MCTS to look for different patterns in the search space.

\begin{figure}[h]
\centering
 \includegraphics[width=8.2cm]{\imgb/mucts_piece_2_14.png}

 \includegraphics[width=8.2cm]{\imgb/sbbs_piece_2_14.png}
 \caption{Examples of MCTS (top) and SBBS (bottom) pieces controlled to have emotion E2.}
 \label{fig:ex_pieces}
\end{figure}

\section{Expressivity Range}

We perform an expressivity analysis to better understand how MCTS is conveying the different target emotions. Expressivity analysis is an evaluation method commonly used in the AI and Games research community to compare different level generators \cite{smith2010analyzing}.  It consists of generating a large set of levels and measuring the biases of the generators according to pre-defined relevant metrics. In the music generation domain,  we use this approach to explore the biases of the MCTS generator when conveying each emotion with respect to human compositions.

% TODO: Compare with human expressivity (maybe run a statistical tests)
We perform the expressivity analysis on the 40 pieces generated by
MCTS for the previous experiments, considering the frequencies of pitch and duration as the metrics to
measure bias. Figure \ref{fig:expressivity} illustrates the distribution of pitch classes and duration types for both MCTS and Human composed pieces. Overal, the MCTS and Human distributions of note durations look similar. For label E0 (high valence and arousal),
Human pieces predominantly have 16th notes, but a few 8th notes are also present. Quarter notes and 32nd notes are rarely used.
MCTS also explored mainly 16th notes in label E0, however it used considerably more 8th and 32nd notes. Label E0 encapsulates emotions such as Happy, Delighted and Excited. These results show that both VGMIDI pieces and MCTS generated pieces with these emotions have rhythm patterns with shorter notes.
% In terms of pitch classes, MCTS used mainly C, C\#, D, E, and A to control pieces in E0.

% TODO: discuss pitch usage
\begin{figure}[h]
\centering

 \includegraphics[width=8.5cm]{\imgb/mcts_human.png}
   \caption{MCTS expressivity range. The x axis represents note duration in seconds and the y axis represents pitch classes. }
 \label{fig:expressivity}
\end{figure}

For label E1 (low valence and high arousal), human compositions have a more even distribution between 16th notes and 8th notes. Quarter notes are present but in a considerably lower frequency and 32nd notes are rarely used. MCTS also used a combination of 16th and 8th notes, but less quarter notes and a little more 32nd notes.
% With respect to pitch classes, MCTS used mainly C D and F. Compared to E0, the variance of note durations and pitches is much lower in E1.
Label E1 represents emotions such as Tense, Angry and Frustrated.
Combining these results with the expressivity range of label E0, we conclude that, VGMIDI and MCTS music with high valence have rhythm
patterns with notes shorter than a quarter note.
% Thus, These results show that, in the VGMIDI dataset, music with these emotions typically uses repetition (as in the ``Jaws'' theme) to create tension in the listener.

For label E2 (low valence and arousal), human  pieces are mainly composed of quarter notes. Pieces with this label also have a few 8th and 16th notes. Different than E0 and E1, E2 has a few longer notes such as half, whole and breve notes. When generating pieces with emotion E2, MCTS also focused on quarter notes. However, compared with human pieces, more shorter notes were used and less longer notes were used. Label E2 encapsulates emotions such as Sad, Depressed and Tired. According to these results, human composed music as well as pieces generated by MCTS on this space tend to have rhythm patters with longer notes.

% This is also an expected result since  Music in this space tends to have a slow tempo and rhythm patterns with long notes. For E2, the pitch classes G, A, and B were the most frequent ones.

For label E3 (high valence and low arousal), human pieces as well as generated pieces have manly 8th notes with a few 16h, quarter and whole notes.  These pieces represent emotions such as Calm, Relaxed, and Content. With these results of expressivity, we conclude that pieces with low arousal have rhythm patters with longer notes.

% In video game music, Calm music (such as the "Sims" theme) tends not to be very slow once the game wants to keep the player engaged. Thus, having this wide combination of durations centered around dotted 8th notes seems a good choice.

\section{Conclusion}

This paper applied MCTS to generate symbolic music with controllable emotion. We used PUCT to explore the search tree, where the probability of the nodes comes from a Music Language Model, and their scores are given by an emotion classifier.
We used a categorical model of emotion with four labels: high valence and arousal; low valence and high arousal; low valence and arousal; and high valence and low arousal.

We evaluated MCTS with two listening tests, one to measure the quality of the generated pieces and one to measure its accuracy in conveying target emotions. In the first experiment, we performed a pairwise comparison between pieces generated by MCTS, SBBS (Beam Search),  TopK sampling, and human composers. Human subjects were asked to rate which pieces they found more musical. In the second experiment, human subjects annotated the generated pieces according to emotion, and we measured the agreement level between MCTS and the annotators. Results showed that MCTS outperforms SBBS in terms of quality and has slightly better accuracy when controlling emotions. With an expressivity analysis, we also show that MCTS generates pieces with expected music features.

To the best of the authors' knowledge, this is the first application of MCTS for music generation. Given that MCTS is agnostic with respect to the model that guides the generation, we believe our approach can be generalized to control other music features. Moreover, since we are using a large vocabulary with approximately 45K tokens, our generative task is similar to other text generation tasks. Thus, we believe our MCTS approach can work well to control text generation.
