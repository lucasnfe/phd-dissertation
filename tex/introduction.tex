% Introduction to Algorithmic Music Composition
Music composers have been using algorithms, rules, and general frameworks for centuries as part of their creative process to compose music \cite{nierhaus2009algorithmic}. For example, Guido of Arezzo (around 991-â€“1031), in his work \textit{Micrologus}, described a system for the automatic conversion of text into melodic phrases. French composers of the \textit{ars nova}, such as Phillipe de Vitry (1291--1361) and Guillaume de Machaut (1300--1377), used isorhythms as a method to map a rhythmic sequence (called \textit{talea}) onto a pitch sequence (called \textit{color}). In the \textit{The Art of the Fugue}, Johann Sebastian Bach (1685--1750) deeply explored contrapuntal compositional techniques such as the fugue and the canon, both being highly procedural.

% TODO: talk about Computational Creativity
Since the 1950s, scientists, engineers, and musicians have been designing algorithms to create computer programs capable of composing music automatically. The \textit{ILLIAC Suite} is considered the first piece to be fully composed automatically by an electronic computer \cite{nierhaus2009algorithmic}. Lejaren Hiller and Leonard Isaacson wrote the program that generated this composition with an ILLIAC computer at the University of Illinois \cite{hiller1957musical}. Since then, many different methods have been proposed to generate music with computers: expert systems \cite{ebciouglu1988expert}, generative grammars \cite{keller2007grammatical}, cellular automata \cite{ball2005making}, evolutionary algorithms \cite{horner1991genetic}, Markov chains \cite{brooks1957}, and neural networks \cite{todd1989connectionist}. The scientific (and artistic) field that organizes these algorithms is called \textit{algorithmic music composition} (AMC). The work in this field has influenced music genres such as generative music \cite{eno1996generative} and supported applications in music analysis \cite{lerdahl1996generative}, procedural audio \cite{farnell2007introduction}, audio synthesis \cite{engel2017neural}, music therapy \cite{williams2020use}, among others.

% Deep Learning and music generation
With the great advances in deep learning since the 2000s, neural networks achieved impressive results in many areas of artificial intelligence (AI) such as computer vision, speech recognition, and natural language processing (NLP) \cite{goodfellow2016deep}. Consequently, AMC researchers started exploring different types of neural networks to generate music: recurrent neural networks (RNNs) \cite{oore2017learning}, transformers \cite{huang2018music}, convolutional neural networks (CNNs) \cite{huang2019counterpoint}, variational autoencoders (VAEs) \cite{roberts2018hierarchical}, generative adversarial networks (GANs) \cite{dong2018musegan}, among others. Inspired by NLP, one of the most common approaches to neural AMC consists of using a transformer or a RNN to build a musical \textit{language model} (LM).

\section{Neural Language Models}

In NLP, a LM is a joint probability function of sequences of tokens (e.g., words or characters) in a language \cite{bengio2003neural}. Modern neural LMs compute the conditional probability of a token $x_t$ given prefix tokens $\{x_1, x_2, \cdots, x_{t-1}\}$ by first computing a dense vector representation (embedding) of the prefix and then feeding it into a classifier to predict the next token \cite{sun2021revisiting}. Neural LMs can be trained from a text corpus and then used to generate new sentences similar to the ones in the corpus. Typically, new sentences are generated in an autoregressive way. Namely, one starts with given prefix tokens $\{x_1, x_2, \cdots, x_{t-1}\}$ which are fed into the LM to generate the next token $x_t$. Next, $x_t$ is concatenated with the prefix, and the process repeats until a special end-of-piece token is found or a given number of tokens are generated. Music can be seen as a sequence of musical tokens (e.g., notes, chords, and parts), and hence a musical LM can be defined to generate music similar to natural LMs. Such musical sequences are typically extracted from a corpus of symbolic music (e.g., MIDI or piano roll) \cite{briot2017deep}. Modern musical LMs have been capable of generating high quality pieces of different styles with strong short-term dependencies\footnote{Supporting strong long-term dependencies (e.g., music form) is still an open problem.} \cite{huang2018music}.

Transformers and RNNs can learn a LM by processing input sequences of tokens $\{x_1, x_2, \cdots, x_{t-1}\}$ to predict, with a \textit{softmax} activation function, an output distribution $\hat{y}_t$ for every token $t$. A \textit{cross-entropy} loss function is then used to compare the predicted probability $\hat{y}_t$ distribution, and the true next word $y_t$. RNNs process sequences step-by-step by keeping an internal state that is updated every step. Transformers process entire sequences in parallel, associating an \textit{attention} score to each token, which determines how much that token contributes to the output of the network. Because transformers process tokens in parallel, they can take advantage of the parallel computing offered by GPUs, and hence can be trained considerably faster than LSTMs \cite{vaswani2017attention}. One drawback of the transformers is that they can only process sentences with a fixed size instead of LSTMs that can process sentences of any size.

\section{Affective Algorithmic Composition}

A major challenge of musical LMs consists of disentangling the trained models to generate compositions with given characteristics \cite{ferreira_2019}. For example, one cannot directly control a LM trained on classical piano pieces to compose a tense piece for a horror scene of a movie. Being able to control the output of the models is especially important for the field of \textit{affective algorithmic composition} (AAC), whose major goal is to automatically generate music that is perceived to have a specific emotion or to evoke emotions in listeners \cite{williams2015investigating}. Applications involve generating soundtracks for movies and video games \cite{williams2015dynamic}, sonification of biophysical data \cite{Chen2015}, and generating responsive music to support music therapy \cite{miranda2011brain}.

The AAC community has explored different ways to control AMC approaches. The traditional AAC methods are typically based on expert systems, evolutionary algorithms, and Markov chains. These methods require rules encoded by music experts to model principles from music theory to control the emotion of generated music. These methods are helpful in systematically investigating how a small combination of music features evoke emotions. However, due to the large space of features (e.g., tempo, melody, harmony, rhythm, timbre, and dynamics), it is challenging to create a fixed set of rules that consider all features. Data-driven methods (e.g., neural networks) do not have this problem because musical rules are learned directly from music data. The challenge with data-driven approaches is that it is relatively expensive to create datasets of music labeled according to a model of emotion. Thus, deep learning for AAC is still in its early days, and this dissertation is part of the first works in this area.

\section{Contributions}

This dissertation explores how to control neural LMs to generate music with a target emotion. Given the limitation of labeled data, the focus of this work is on search-based methods that use a music emotion classifier to steer the distribution of pre-trained musical LMs. With this framing, a high capacity LM $L$ is pre-trained with a large unlabelled dataset and a music emotion classifier $E$ is trained with the labeled data to predict emotions $e$. In order to boost the accuracy of the emotion classifier $E$, it is trained with transfer learning by fine-tuning the LM $L$ with an additional classification layer. Three different search-based approaches have been proposed to control the LM $L$ with the emotion classifier $E$ to generate pieces with a target emotion $e$.

% ISMIR19
\subsection{Learning to Generate Music with Sentiment}

Inspired by the work of \citet{radford_2017}, the first explored approach is a genetic algorithm that optimizes the neurons of $L$ that carry sentiment signal (positive or negative)\footnote{In this first work, only sentiment (and not emotions) was considered to simplify the problem.}, as given by $E$. A reasonably large labeled dataset called VGMIDI was created to train both $L$ and $E$. All pieces in the dataset are piano arrangements of video game soundtracks. A custom web tool was designed to label these piano pieces according to the circumplex (valence-arousal) model of emotion \cite{russell1980circumplex}. Labeling music pieces according to emotion is a subjective task. Therefore, the pieces were annotated by 30 annotators via Amazon Mechanical Turk (MTurk), and the mean of these annotations was considered the ground truth. In this first work, the VGMIDI dataset had 95 labeled pieces and 728 unlabelled ones. The LM $L$ was modeled as a long short-term memory (LSTM) network pre-trained with these 728 unlabelled pieces. The sentiment classifier $E$ was trained by fine-tuning $L$ with an extra linear layer on the 95 labeled pieces.

L1 regularization was used while training $E$ to enforce a sparse set of weights in $E$. This regularization highlighted the subset of neurons in $L$ that carry sentiment signal. Thus, a genetic algorithm was used to optimize the weights of these L1 neurons to lead $L$ to generate either positive or negative pieces. This approach was evaluated with a listening test where annotators labeled three pieces generated to be positive and three pieces generated to be negative. Results showed that the annotators agree that pieces generated to be positive are indeed positive. However, pieces generated to be negative are a little ambiguous, according to the annotators. This work was published in the Proceedings of the 20th Conference of the International Society for Music Information Retrieval (ISMIR19) \cite{ferreira_2019}.

% AIIDE20
\subsection{Computer-Generated Music for Tabletop Role-Playing Games}

The second approach is a variation of beam search, called \textit{stochastic bi-objective beam search} (SBBS), to decode the outputs of $L$ with the guidance of $E$ into a sequence of musical tokens that convey $e$. Unlike the first approach, SBBS does not update the $L$ weights to control $L$ towards $e$. Instead, it steers the probability distribution of $L$ in generation time by multiplying the probabilities of $L$ with $E$. In this work, $E$ is implemented as two independent binary classifiers: $E_v$ for valence and $E_a$ for arousal. At every decoding step, SBBS samples the next \textit{beam} (set of candidate solutions) from this resulting distribution. SBBS applies \textit{top k} filtering when expanding the search space in order to control the quality of the generated pieces.

In this work, the VGMIDI dataset was extended with extra 105 labeled pieces, increasing the number of labeled pieces to 200. Moreover, a new dataset of unlabelled piano pieces, called ADL Piano Midi, was created to train a larger $L$. ADL Piano Midi is composed of 11,086 piano pieces from different genres, where 9,021 of them were extracted from the Lakh MIDI dataset \cite{raffel2016learning} and 2,065 were scraped from publicly available sources on the internet. The LM $L$ was implemented with a GPT2 \cite{radford2019language} transformer network pre-trained with the ADL Piano Midi. The emotion classifiers $E_v$ and $E_a$ were both trained by fine-tuning $L$ with an extra linear layer on the 200 labeled pieces of the VGMIDI dataset.

SBBS was evaluated in the context of tabletop role-playing games. A system called \textit{Bardo Composer} was built with SBBS to generate background music for game sessions of Dungeons \& Dragons. Bardo Composer uses a speech recognition system to translate player speech into text, which is classified as having an emotion $e$. Bardo Composer then uses SBBS to generate musical pieces conveying the target emotion $e$. A user study showed that human subjects correctly identified the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces composed by humans. The contributions of this work were published in the Proceedings of the 13th \cite{padovani2017} and 16th \cite{ferreira2020computer} AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE17 and AIIDE20).

% Beam search is one of the standard methods to decode LMs in text generation tasks (e.g., machine translation).
% Traditionally, beam search explores the space of sentences defined by a LM for by maximizing the probability as given by the LM.

% ISMIR21
\subsection{Controlling Emotions in Symbolic Music Generation with MCTS}

% SBBS is a relatively fast way to decode a LM $L$ to generate music with a target emotion $e$. However, it can generate repetitive music since that maximizes both the probabilities of $L$ and the $E$. In order to improve the quality of the generated music,

The third and most recent approach is another decoding algorithm that, similar to SBBS, does not update the $L$ weights to control $L$ towards the emotion $e$. This new decoding algorithm is based on \textit{monte carlo tree search} (MCTS). At every step of the decoding process, MCTS uses \textit{predictor upper confidence for trees} (PUCT) to search over the space of sequences defined by $L$ for solutions that maximize the average values of emotion given by $E$. MCTS samples from the distribution of node visits created during the search to decode the next token.

In this work, the VGMIDI dataset was extended with extra 2,912 unlabeled pieces, increasing the number of unlabeled pieces to 3,640. The LM $L$ was implemented with a \textit{music transformer} \cite{huang2018music} pre-trained with these 3,640 unlabelled pieces. Unlike the second work, where $E$ was split into two binary classifiers, $E$ was trained as a single multiclass emotion classifier. Like the previous works, $E$ was trained by fine-tuning $L$ with an extra linear layer on the 200 labeled pieces of the VGMIDI dataset.

Two listening tests were performed to evaluate MCTS. The first one evaluates the quality of generated pieces, and the second one evaluates the MCTS accuracy in generating pieces with a given emotion. Results showed that MCTS is as good as SBBS in controlling emotions while improving music quality. An expressivity analysis of the generated pieces was also performed to show the music features being used to convey each emotion. The frequencies of pitch classes and note durations suggest that MCTS can reproduce some common composition practices used by human composers.

% Contributions
% This dissertation has multiple contributions. The first one is the VGMIDI dataset, one of the first datasets of symbolic music labeled according to emotion. Framing the problem of music generation with controllable emotion as steering the distribution of music LMs with an emotion classifier is also a contribution. The three search methods explored are also significant contributions of this dissertation, once they present the first approaches to solve the proposed problem. Finally, the system Bardo Composer system designed to evaluate the SBBS method is another contribution for being the first system to compose music for tabletop role-playing games. These contributions open new possibilities of research that can be explored in the future, from extending the VGMIDI dataset to developing new neural models and applying Bardo Composer to generate soundtracks for other media (e.g., videos games, audio books, and films).

% First of all, different neural network architectures, such as VAEs, can be designed to control perceived emotion in music. VAEs have the benefit of learning disentangled representations of music \cite{yang2019deep}, so they can be a good approach to control emotion in music generation. Second, one can use different search methods to steer the distribution of the language model. For example, new MCTS or SBBS algorithm variations can be designed to improve music quality while keeping the desired target emotion. Third, increasing the VGMIDI dataset can considerably improve the accuracy of the music emotion classifier. This can support the design of deeper neural networks that can potentially be better music generators with controllable emotions. Finally, all these contributions allow different applications, such as composing music in real-time for tabletop role-playing games. One can extend Bardo Composer to create soundtracks for other experiences such as spoken poetry, bedtime stories, video games and movies.

\section{Dissertation Outline}

% Outline
This dissertation is organized as follows: Chapters \ref{ch:amc} and \ref{ch:ml} present the background work that this dissertation builds upon. While Chapter \ref{ch:amc} presents an overview of algorithmic music composition, Chapter \ref{ch:ml} dives into the fundamentals of deep learning for music generation. Chapter \ref{ch:related} reviews the previous methods to control the emotion of music composed algorithmically. It also reviews techniques developed within the NLP community to control LMs for different text generation tasks. Chapter \ref{ch:ismir19} describes the work published at ISMIR19, where a genetic algorithm is used to fine-tune a pre-trained LSTM. Chapter \ref{ch:aiide20} presents the work published at AIIDE20, which uses SBBS within Bardo Composer to compose music for tabletop roleplaying games. Chapter \ref{ch:ismir21} presents this dissertation's most recent contribution: an MCTS decoding algorithm to control LMs to generate music with a target emotion. Chapter \ref{ch:future} discusses the weakness of the methods proposed in this dissertation in order to highlight different directions of future work. Finally, Chapter \ref{ch:conclusion} concludes this dissertation.
