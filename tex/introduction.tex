% Introduction to Algorithmic Music Composition
Music composers have been using algorithms, rules and general frameworks for centuries as part of
their creative process to compose music \cite{nierhaus2009algorithmic}. For example, Guido of Arezzo
(around 991-â€“1031), in his work ``Micrologus'', describes a system for the automatic conversion of
text into melodic phrases. French composers of the \textit{ars nova}, such as Phillipe de Vitry (1291--1361) and
Guillaume de Machaut (1300--1377), used isorhythms as a method to map a rhythmic sequence
(called \textit{talea}) onto a pitch sequence (called \textit{color}). In the ``The Art of the Fugue'',
Johann Sebastian Bach (1685--1750) deeply explores contrapuntal compositional techniques such as
the fugue and the canon, both being highly procedural.

% TODO: talk about Computational Creativity
Since the 1950s, scientists, engineers and musicians have been designing algorithms to create computer
programs capable of composing music automatically. The ``ILLIAC Suite'' is considered to be
the first piece to be fully composed automatically by an electronic computer \cite{nierhaus2009algorithmic}.
The program that generated this composition was written by Lejaren Hiller and Leonard Isaacson
for the ILLIAC computer at the University of Illinois \cite{hiller1957musical}.
Since then, many different methods have been proposed to generate music with computers:
expert systems \cite{}, generative grammars \cite{}, cellular automata \cite{},
evolutionary algorithms \cite{}, markov models \cite{}, and neural networks \cite{}.
The scientific (and artistic) field that organizes these algorithms is called Algorithmic Music Composition.
The work in this field has influenced different music genres such as Generative Music
\cite{eno1996generative} and supported applications in music analysis
\cite{lerdahl1996generative}, procedural audio \cite{farnell2007introduction},
audio synthesis \cite{engel2017neural}, music therapy \cite{williams2020use}, etc.

% Deep Learning and music generation
With the great advances in deep learning since the 2000s, neural networks achieved impressive results in many areas of artificial intelligence (AI) such as computer vision (CV), speech recognition and natural language processing (NLP) \cite{goodfellow2016deep}. Consequently, algorithmic music composition researchers started exploring different types of neural networks to generate music: recurrent neural networks (RNNs) \cite{oore2017learning}, transformers \cite{huang2018music}, convolutional neural networks (CNNs) \cite{huang2019counterpoint}, variational autoencoders (VAEs) \cite{roberts2018hierarchical}, generative adversarial networks \cite{dong2018musegan}, etc. One of the most common approaches of neural-based music generation consists of using a transformer (or a RNN) to build a neural music language model (LM) that computes the likelihood of the next musical symbols (e.g., note) in a piece. Typically, the symbols are extracted from MIDI or piano roll representations of music \cite{briot2017deep}. Music is then generated by sampling from the distribution learned by the LM. Such models have been capable of generating high quality pieces of different styles with strong short-term dependencies. Supporting strong long-term dependencies (e.g. music form) is still an open problem \cite{briot2017deep}.

% Affective Algorithmic Music Composition
A major challenge of music LMs consists of disentangling the trained models to generate compositions with given characteristics \cite{ferreira_2019}. For example, one cannot directly control a model trained on classical piano pieces to compose a tense piece for a horror scene of a movie. Being able to control the output of the models is especially important for the field of affective algorithmic music composition, whose major goal is to automatically generate music that is perceived to have a specific emotion or to evoke emotions in listeners \cite{williams2015investigating}. Applications involve generating soundtracks for movies and video games \cite{williams2015dynamic}, sonification of biophysical data \cite{Chen2015}, and generating responsive music for the purposes of music therapy and palliative care \cite{miranda2011brain}.

One of the most common approaches to affective algorithmic music composition is rule-based systems \cite{williams2015investigating}, which use rules encoded by music experts to model principles
from music theory to control the emotion of generated music. These systems are helpful in systematically investigating how a small combination of music features (tempo, melody, harmony, rhythm, timbre, dynamics, etc.) evoke emotions. However, due to the large space of features, it is challenging to create a fixed set of rules that consider all music features. Learning-based methods do not have this problem because one does not need to specify the rules to map music features into emotion. These rules are learned directly from music data.

% Contributions
This dissertation explores how to control LMs to generate music with a target perceived emotion. Emotion recognition is an important topic in Music Information Retrieval (MIR) \cite{kim2010music}, but it is typically studied with waveform representation of music. Thus, a reasonably large labeled dataset called VGMIDI has been created to support this research. All pieces in the dataset are piano arrangements of video game soundtracks. A custom web tool has been designed to label these piano pieces according to a valence-arousal model of emotion \cite{russell1980circumplex}. Labeling music pieces according to emotion is a subjective task. Thus pieces were annotated by 30 annotators via Amazon Mechanical Turk and the average of these annotations is considered the ground truth. Due to its subjectivity, this annotation task is considerably expensive and hence the VGMIDI is still a limited dataset with only 200 labelled pieces and 3640 unlabelled ones.

Given the limitation of labeled data, the focus of this work is on search methods that use a music emotion classifier to steer the distribution of a LM. With this framing, a LM is trained with the unlabelled data and a music emotion classifier is trained with the labeled data. Both recurrent neural networks and Transformers were considered as LMs. In order to boost the accuracy of the emotion classifier, it is trained with transfer learning by fine-tuning the LM with an additional classification head. Three different search approaches have been explored to control the LM with the emotion classifier: Genetic Algorithms, Beam Search and Monte Carlo Tree Search (MCTS).

% ISMIR19
Inspired by the work of Radford et al. \cite{radford_2017}, the first explored approach is a genetic algorithm that optimizes the neurons in the LM that carry sentiment signal when fine-tuned with a classification head. In this first work, only the valence (sentiment) dimension of the labeled pieces was used to simplify the problem. The LM is a long short-term memory (LSTM) neural network trained with the 728 unlabelled pieces of the first version of the VGMIDI dataset. The emotion classifier is a single linear layer stacked on top of LM, which is fine-tuned with the 95 labeled pieces of the first version of the VGMIDI dataset. This approach was evaluated with a listening test where annotators labeled three pieces generated to be positive and three pieces generated to be negative. Results showed that the annotators agree that pieces generated to be positive are indeed positive. However, pieces generated to be negative are a little ambiguous according to the annotators.

% AIIDE20
The second approach is a variation of beam search called Stochastic Bi-Objective Beam Search (SBBS). Beam search is one of the most common methods to decode LMs in text generation tasks such as machine translation. Traditionally, Beam Search searches for sentences that maximize the probability as given by the LM. Aligning with the work of Holtzman et al. \cite{holtzman2018learning}, SBBS guides the generative process towards a given emotion by multiplying the probabilities of the LM with the probabilities of two binary classifiers (one for valence and one for arousal). At every decoding step, SBBS samples the next beam from this resulting distribution. SBBS applies \textit{top k} filtering when expanding the search space in order to control the quality of the generated pieces. SBBS was evaluated in the context of tabletop role-playing games. A system called Bardo Composer was built with SBBS to generate background music for game sessions of the Dungeons \& Dragons game. A user study showed that human subjects correctly identified the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces composed by humans.

% ISMIR21
SBBS is a relatively fast way to decode LMs to generate music. However, it can generate much repetitive music since that maximizes both the probabilities of the language model and the emotion model. In order to improve the quality of the generated music, the third method is a monte carlo tree search (MCTS) that, at every step of the decoding process, use Predictor upper confidence for trees (PUCT) to search for sequences that maximize the average values of emotion given by the emotion classifier. MCTS samples from the distribution of node visits created during the search to decode the next token. Two listening tests were performed to evaluate this method. The first one evaluates the quality of generated pieces and the second one evaluates the MCTS accuracy in generating pieces with a given emotion. An expressivity analysis of the generated pieces was also performed to show the music features being used to convey each emotion. Results showed that MCTS is as good as SBBS  in controlling emotions while improving music quality.

% Contributions
This dissertation has multiple contributions. The first one is the VGMIDI dataset, one of the first datasets of symbolic music labeled according to emotion. Framing the problem of music generation with controllable emotion as steering the distribution of music LMs with an emotion classifier is also a contribution. The three search methods explored are also significant contributions of this dissertation, once they present the first approaches to solve the proposed problem. Finally, the system Bardo Composer system designed to evaluate the SBBS method is another contribution for being the first system to compose music for tabletop role-playing games.

% Future works
These contributions open new possibilities of research that can be explored in the future. First of all, different neural network architectures, such as VAEs, can be designed to control perceived emotion in music. VAEs have the benefit of learning disentangled representations of music \cite{yang2019deep}, so they can be a good approach to control emotion in music generation. Second, one can use different search methods to steer the distribution of the language model. For example, new MCTS or SBBS algorithm variations can be designed to improve music quality while keeping the desired target emotion. Third, increasing the VGMIDI dataset can considerably improve the accuracy of the music emotion classifier. This can support the design of deeper neural networks that can potentially be better music generators with controllable emotions. Finally, all these contributions allow different applications, such as composing music in real-time for tabletop role-playing games. One can extend Bardo Composer to create soundtracks for other experiences such as spoken poetry, bedtime stories, video games and movies.
