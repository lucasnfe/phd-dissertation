\section{Introduction}

This chapter presents \textit{Bardo Composer}, or \textit{Composer} for short, the second major contribution of this dissertation. Bardo Composer is a system to generate background music for tabletop role-playing games (TRPG). It uses a speech recognition system to translate player speech into text, which is classified by a transformer NN according to a categorical model of emotion. A second transformer NN is trained to classify the emotion of symbolic piano pieces. Bardo Composer then uses Stochastic Bi-Objective Beam Search, a new decoding strategy inspired by Holtzman et al. \cite{holtzman2018learning}, to generate musical pieces conveying the desired emotion. Stochastic Bi-Objective Beam Search works by steering the probabilities given by the LM with the probabilities given by the music emotion classifier. A user study with 116 participants was performed to evaluate whether people are able to correctly identify the emotion conveyed in pieces generated by the system. The evaluated pieces were generated for Call of the Wild (CotW), a Dungeons and Dragons campaign available on YouTube. Results show that human subjects could correctly identify the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces written by humans. This work has been published at the 20th annual conference of the International Society for Music Information Retrieval (ISMIR).

Bardo Composer is based on a previous system called \textit{Bardo}, presented by \citet{padovani2017} to select pre-authored background music for tabletop games. Bardo uses a \textit{naive bayes} approach to classifying sentences produced by a speech recognition system into one of the four emotions: Happy, Calm, Agitated, and Suspenseful. Bardo then selects a music piece from a library corresponding to the classified emotion. The selected piece is then played as background music whenever the naive bayes classifier detects an emotion transition in the story. Composer use \citet{padovani2017}'s dataset to train an emotion classifier for the story being told at a game session. Their dataset includes 9 episodes of CotW, which contains 5,892 sentences and 45,247 words, resulting in 4 hours, 39 minutes, and 24 seconds of gameplay. There are 2,005 Agitated, 2,493 Suspenseful, 38 Happy, and 1,356 Calm sentences in the dataset.

Composer uses a discretized circumplex emotion model \cite{russell1980circumplex} that generalizes the model used in Bardo. In Composer's emotion model, the dimensions of valence and arousal, denoted by a pair $(v,a)$, assume  binary values $v \in [0, 1]$ and $a \in [0, 1]$, respectively. Valence measures sentiment and thus $v = 0$ means a negative input and $v = 1$ means a positive input. Arousal measures the energy of the input, and thus $a = 0$ means that the input has low energy, whereas $a = 1$ means that the input has high energy. This model is used for classifying both the emotion of the player's speeches and the emotion of the generated music.

\section{Bardo Composer}

A general overview of Composer is shown in Algorithm~\ref{alg:bardo}. It receives as input a speech recognition system $S$, an emotion classifier for text $E_s$, an emotion classifier for music $E_m$, a LM for symbolic music generation $L$, a speech signal $v$ with the last sentences spoken by the players, and a sequence $x$ of musical symbols composed in previous calls to Composer. The algorithm also receives parameters $b$ and $k$, which are used in the search procedure described in Algorithm ~\ref{alg:sbs}. Composer returns a symbolic piece that tries to match the emotion in the players' speeches.

\begin{algorithm}[t]

\caption{Bardo Composer}
\label{alg:bardo}
\begin{algorithmic}[1]
\REQUIRE Speech recognition system $S$, Text emotion classifier $E_s$, Music emotion classifier $E_m$, LM $L$, speech signal $v$, previously composed symbols $x$, beam size $b$, number of symbols $k$
\ENSURE Music piece $x$
\STATE $s, l \gets S(v)$ \label{line:voice2text}
\STATE $v, a \gets E_s(s)$ \label{line:emotion_classification}
% \STATE $x \gets \{\}$ \label{line:init}
%\STATE $i \gets 0$
%\WHILE{time of play of $[x_{t}, \cdots, x_{t + i}]$ is lower than $k$} \label{line:generation1}
\STATE $y \gets$ \texttt{SBBS}$(L, E_m, x, v, a, b, k, l)$ \# \emph{see Algorithm~\ref{alg:sbs}} \label{line:sbs}
\RETURN $x \cup y$ \label{line:generation2}
%\STATE $i \gets i + 1$
%\ENDWHILE
%\RETURN $x$
\end{algorithmic}
\end{algorithm}

Composer starts by converting the speech signal $v$ into text $s$ with $S$ (line~\ref{line:voice2text}). In addition to text, $S$ returns the duration of the signal $v$ in seconds, this is stored in $l$. Then, Composer classifies the emotion of $s$ in terms of valence $v$ and arousal $a$ and it
%initializes an empty sequence $x$ of musical symbols (line~\ref{line:init}) and it
invokes Stochastic Bi-Objective Beam Search (\texttt{SBBS}) to generate a sequence of symbols $y$ that matches the desired length $l$ and emotion with arousal $a$ and valence $v$.
%the next symbol in the piece by invoking a Stochastic Beam Search (\texttt{SBBS}) procedure, until the generated piece matches the input voice signal in terms of length (line~\ref{line:generation1}).
\texttt{SBBS} receives as input the models $L$ and $E_m$, the current sequence $x$, the desired emotion values $v$ and $a$, \texttt{SBBS}'s parameter values $b$ and $k$, which are explained below, and the desired length $l$ of the piece to be generated. %\texttt{SBBS} is described below.

In the first call to Composer, the sequence $x$ is initialized with the symbols of the first 4 timesteps of a random human-composed piece with the emotion $v, a$, as returned by $E_s$. Every time there is a transition from one emotion to another, the sequence $x$ is reinitialized using the same process. This is used to bias the generative process and to emphasize emotion transitions. To be used in real-time, Composer is invoked with the most recently captured speech signal $v$ and returns a composed piece of music. While the most recent piece is being played at the game table, Composer receives another signal $v$ and composes the next excerpt. One also needs to define the length of the signal $v$. Similar to Bardo \citet{padovani2017}, Composer uses YouTube's subtitle system as the speech recognition system $S$. Therefore, signals $v$ are long enough to form a sentence in the form of a subtitle.

\subsection{Classifying the Story's Emotion}

In order to have a common model of emotion between stories and music, Bardo's four emotion model is mapped to the circumplex model used by Composer:

\begin{enumerate}
    \item Suspenseful is mapped to low valence and arousal ($v = 0, a = 0$).
    \item Agitated is mapped to low valence and high arousal ($v = 0, a = 1$).
    \item Calm is mapped to high valence and low arousal ($v = 1, a = 0$).
    \item Happy is mapped to high valence and arousal ($v = 1, a = 1$).
\end{enumerate}

The valence-arousal mapping is based on the model used to annotate the VGMIDI dataset. When human subjects annotated that dataset, they used a continuous valence/arousal model with labels defining a fixed set of discrete basic emotions (e.g. happy or sad) \cite{ferreira_2019}. This mapping allows one to use the circumplex model with the labeled CotW dataset. For example, in the context of the game Dungeons and Dragons, the sentence ``Roll initiative'' is normally said at the beginning of battles and it can be considered $(v = 0, a = 1)$, once a battle is a negative (dangerous) moment with high energy. ``Roll initiative'' is normally classified as Agitated in \citet{padovani2017}'s dataset.

Given the limited amount of TRPG stories labeled according to emotion (there are only 5,892 sentences in the CotW dataset), the sentences \cite{Radford2018} are classified with a transfer learning approach. A high capacity BERT \cite{devlin2018bert} model is fine-tuned with the CotW dataset. BERT is used because it is currently one of the state-of-the-art architectures across different NLP tasks \cite{devlin2018bert}. Although in Algorithm~\ref{alg:bardo} the classifier for story emotions is depicted as a single $E_s$ model, valence and arousal are treated independently in practice, thus a pre-trained BERT is fine-tuned for each dimension.

\subsection{Classifying the Music's Emotion}

As was the case with the TRPG stories, given the limited amount of MIDI pieces labeled according to emotion, the emotion music classifier ($E_m$) is also trained with a transfer learning approach. However, different than the $E_s$, where a BERT model is fine-tuned, $E_m$ uses a fine-tuned GPT-2 model. $E_m$ uses a GPT-2 because it is better suited for sequence generation than BERT. Similar to $E_s$, $E_m$ also treats valence and arousal independently. Thus, a pre-trained GPT-2 is fine-tuned for each of these dimensions.

Currently, in the symbolic music domain, there is no publicly available high-capacity LM pre-trained with large (general) datasets. Typically, models in this domain are trained with relatively small and specific datasets. For example, the MAESTRO dataset \cite{hawthorne2018enabling}, the Bach Chorales \cite{hadjeres2017deepbach} and the VGMIDI \cite{ferreira_2019} dataset. Therefore, a general high-capacity GPT-2 has been pre-trained as a LM \cite{radford2019language} using a new dataset called ADL (Augmented Design Lab) Piano MIDI dataset \footnote{https://github.com/lucasnfe/adl-piano-midi}. The ADL Piano MIDI dataset is based on the Lakh MIDI dataset \cite{raffel2016learning}, which is one of the largest MIDI datasets publicly available. Among these files, there are many versions of the same piece. Only one version of each piece was kept. Given that the VGMIDI dataset is limited to piano pieces, only the tracks with instruments from the piano family (MIDI program numbers 1-8 in the dataset) were considered from the Lakh MIDI dataset. This process generated a total of 9,021 unique piano MIDI files. These files are mainly Rock and Classical pieces, so to increase the genre diversity (e.g. Jazz, Blues, and Latin) of the dataset, an additional 2,065 files were included from public sources on the Internet\footnote{\url{https://bushgrafts.com/midi/} and \url{http://midkar.com/jazz/}}. All files in the final collection were de-duped according to their MD5 checksum. The final dataset has 11,086 pieces.

After pre-training the high-capacity GPT-2 model, two independent models were fine-tuned (one for valence and one for arousal) with an extended version of the VGMIDI dataset\cite{ferreira_2019}. Following the approach of \citet{Radford2018}, these models were fine-tuned by adding and extra layer to the pre-trained LM and training the entire model (including the pre-trained layers) with the VGMIDI dataset~\cite{ferreira_2019}. In this work, the VGMIDI dataset has been extended from 95 to 200 labeled pieces using the same annotation method of the original dataset. All the 200 pieces are piano arrangements of video game soundtracks labeled according to the circumplex model of emotion.

\subsubsection{Encoding}

Composer encodes a MIDI file by parsing all notes from the \texttt{NOTE\_ON} and \texttt{NOTE\_OFF} events in the MIDI. A note is defined as a set $z = (z_p, z_s, z_d, z_v)$, where $\{z_p \in \mathbb{Z} \vert 0 \leq z_p < 128 \}$ is the pitch number, $\{z_s \in \mathbb{Z} \vert z_s \geq 0 \}$ is the note starting time in timesteps,  $\{z_d \in \mathbb{Z} \vert 0 \leq z_d \leq 56\}$ is note duration in timesteps and $\{z_v \in \mathbb{Z} \vert 0 \leq z_v < 128 \}$ is the note velocity. Given a MIDI \texttt{NOTE\_ON} event, a note $z$ is parsed by retrieving the starting time $z_s$ (in seconds), the pitch number $z_p$ and the velocity $z_v$ from that event. To calculate the note duration $z_d$, the end time $z_e$ (in seconds) of the correspondent \texttt{NOTE\_OFF} event is retrieved. Thus, the note duration $z_d = \floor{t \cdot z_e} - \floor{t \cdot z_s}$ is computed from the  discretized durations $z_s$ and $z_e$, where $t$ is a parameter defining the sampling frequency of the timesteps.
Composer derives a sequence $x = \{z_v^1, z_{d}^1, z_{p}^1, \cdots, z_v^n, z_{d}^n, z_p^n\}$ of tokens for a given MIDI file by (a) parsing all notes $z^i$ from the file, (b) sorting them by starting time $z_s^j$ and (c) concatenating their velocity $z_v^j$, duration $z_d^j$ and pitch $z_p^j$. Composer adds two special tokens \texttt{TS} and \texttt{END} in the sequence $x$, to mark the end of a timestep and the end of a piece, respectively. This encoding yields a vocabulary $V$ of size $|V| = 314$.
% Figure \ref{fig:encoding} illustrates an example of MIDI file encoded using this approach.

\subsection{Stochastic Bi-Objective Beam Search}

Composer uses a new search-based decoding algorithm called \textit{Stochastic Bi-Objective Beam Search} (\texttt{SBBS}), which combines a LM and a music emotion classifier to bias the process of music generation to match a target emotion (line \ref{line:sbs} of Algorithm \ref{alg:bardo}). Given a LM $L$ and the music emotion classifiers $E_{m, v}$ and $E_{m, a}$, for valence and arousal, respectively. The goal of \texttt{SBBS} is to allow for the generation of pieces that sound ``good'' (i.e., have high probability value according to the trained LM), but that also match the current emotion of the story being told by the players. \texttt{SBBS} is stochastic because it samples from a distribution instead of greedily selecting the best sequences of symbols, as a regular beam search does. The stochasticity of \texttt{SBBS} allows it to generate a large variety of musical pieces for the same values of $v$ and $a$. \texttt{SBBS} is ``bi-objective'' because it optimizes for realism and emotion.
The pseudocode of \texttt{SBBS} is shown in Algorithm~\ref{alg:sbs}. The letters $x, y$ and $m$ denote sequences of musical symbols. Function $p_L(y) = \prod_{y_t \in y} P(y_t|y_0, \cdots, y_{t-1})$ is the probability of sequence $y$ according to the LM $L$; a high value of $p_L(y)$ means that $y$ is recognized as a piece of ``good quality'' by $L$. Function $l(y)$ denotes the duration in seconds of piece $y$. Finally, $x[i:j]$ denotes the subsequence of $x$ starting at index $i$ and finishing at index $j$.

\begin{algorithm}[t]
\caption{Stochastic Bi-Objective Beam Search}
\label{alg:sbs}
\begin{algorithmic}[1]
\REQUIRE Music emotion classifier $E_m$, LM $L$, previously composed symbols $x$, valence and arousal values $v$ and $a$, number $k$ of symbols to consider, beam size $b$, length $l$ in seconds of the generated piece.
\ENSURE Sequence of symbols of $l$ seconds.
\STATE $B \gets [x]$, $j \gets 0$ \label{line:sbs:init}
% \FOR{$i = 1$ to $b$}
% \STATE $B$.append$(x \cup x_t)$ where $x_t \sim P(\cdot|x)$
% \ENDFOR
\WHILE{$l(y[t:t+j]) < l$, $\forall y \in B$} \label{line:sbs:stopping_condition}
    \STATE $C \gets \{\}$ \label{line:sbs:init_while}
    \FORALL{$m \in B$}
        \STATE $C_m \gets \{m \cup s \vert s \in V\}$ \label{line:sbs:children}
        \STATE $C_m \gets k$ elements $y$ from $C_m$ with largest $p_L(y)$ \label{line:sbs:pruning_model}
        \STATE $C \gets C \cup C_i$ \label{line:sbs:total_children}
    \ENDFOR
    \STATE $B \gets b$ sequences $y$ sampled from $C$ proportionally to $p_L(y) (1 - |v - E_{m,v}(y)|) (1 - |a - E_{m,a}(y)|)$ \label{line:sbs:sample_next_beam}
    \STATE $j \gets j + 1$ \label{line:sbs:end_while}
    % \begin{equation*}
    % p(y) \cdot (1 - |v - E_{m,v}(y)|) \cdot (1 - |a - E_{m,a}(y)|) \,.
    % \end{equation*}
\ENDWHILE
\RETURN $m \in B$ such that $p_L(m) = \max_{y \in B}p_L(y)$ and $l(y[t: t+j]) \geq l$ \label{line:sbs:return}
\end{algorithmic}
\end{algorithm}

\texttt{SBBS} initializes the beam structure $B$ with the sequence $x$ passed as input (line \ref{line:sbs:init}).
\texttt{SBBS} also initializes variable $j$ for counting the number of symbols added by the search. \texttt{SBBS} keeps in memory at most $b$ sequences and, while all sequences are shorter than the desired duration $l$ (line \ref{line:sbs:stopping_condition}), it adds a symbol to each sequence (lines \ref{line:sbs:init_while}--\ref{line:sbs:end_while}). \texttt{SBBS} then generates all sequences by adding one symbol from vocabulary $V$ to each sequence $m$ from $B$ (line \ref{line:sbs:children}); these extended sequences, known as the children of $m$, are stored in $C_m$. The operations performed in lines \ref{line:sbs:pruning_model} and \ref{line:sbs:sample_next_beam} attempt to ensure the generation of good pieces that convey the desired emotion. In line \ref{line:sbs:pruning_model}, \texttt{SBBS} selects the $k$ sequences with largest $p_L$-value among the children of $m$. This is because some of the children with low $p_L$-value could be attractive from the perspective of the desired emotion and, although the resulting piece could convey the desired emotion, the piece would be of low quality according to the LM. The best $k$ children of each sequence in the beam are added to set $C$ (line \ref{line:sbs:total_children}). Then, in line \ref{line:sbs:sample_next_beam}, \texttt{SBBS} samples the sequences that will form the beam of the next iteration. Sampling occurs proportionally to the values of $p_L(y) (1 - |v - E_{m,v}(y)|) (1 - |a - E_{m,a}(y)|)$, for sequences $y$ in $C$. A sequence $y$ has higher chance of being selected if $L$ attributes a high probability value to $y$ and if the music emotion model classifies the values of valence and arousal of $y$ to be similar to the desired emotion. When at least one of the sequences is longer than the desired duration of the piece, \texttt{SBBS} returns the sequence with largest $p_L$-value that satisfies the duration constraint (line~\ref{line:sbs:return}).

\section{Empirical Evaluation}

SBBS is empirically evaluated with two experiments. The first one evaluates the accuracy of the models for story and music emotion classification. The fine-tuned BERT model for story emotion classification is compared against the simpler Na\"ive Bayes approach of \citet{padovani2017}. The fine-tuned GPT-2 model for music emotion classification is compared against the simpler LSTM of \cite{ferreira_2019}. The second experiment evaluates, with a user study, whether human subjects can recognize different emotions in pieces generated by Composer for the CotW campaign.

\subsection{Emotion Classifiers}

\subsubsection{Story Emotion}

Composer's story emotion classifier is a pair of pre-trained BERT$_{BASE}$ models \cite{devlin2018bert}, one for valence and one for arousal. BERT$_{BASE}$ has 12 layers, 768 units per layer, and 12 attention heads. BERT$_{BASE}$ was pre-trained using both the BooksCorpus (800M words) \cite{zhu2015aligning} and the English Wikipedia (2,500M words). These two BERT models are independently fine-tuned as valence, and arousal classifiers using the CotW dataset \cite{padovani2017}. Fine-tuning consists of adding a classification head on top of the pre-trained model and training all the parameters (including the pre-trained ones) of the resulting model end-to-end. All these parameters were fine-tuned for 10 epochs with the Adam optimizer~\cite{adam14}. Each training step was performed over a mini-batch of size 32. The learning rate was set to 3e-5 and the dropout rate to 0.5. The CotW dataset is divided into 9 episodes. Thus, the accuracy of each BERT classifier is evaluated with a leave-one-out strategy. Given a set of episodes $E$, for each episode $e \in E$, the $E - e$ episodes are used for training, and the episode $e$ is used for testing. For example, when testing on episode 1, episodes 2-8 are used for training. Every sentence is encoded using a WordPiece embedding \cite{wu2016google} with a 30,000 token vocabulary.

The fine-tuned BERT classifiers are compared with a Na\"ive Bayes (NB) approach (baseline), chosen because it is the method underlying the original Bardo system. NB encodes sequences using a traditional bag-of-words with tf–idf approach. Table~\ref{tab:valence} shows the accuracy of the valence classification of both these methods per episode. The best accuracy for a given episode is highlighted in bold. The BERT classifier outperforms NB in all the episodes, having an average accuracy 7\% higher. For valence classification, the hardest episode for both the models is episode 7, where BERT had the best performance improvement when compared to NB. The story told in episode 7 of CotW is different from all other episodes. While the other episodes are full of battles and ability checks, episode 7 is mostly the players talking with non-player characters. Therefore, what is learned in the other episodes does not generalize well to episode 7. The improvement in accuracy of the BERT model in that episode is likely due to the model's pre-training. Episodes 5 and 9 were equally easy for both methods because they are similar to one another. The system trained in one of these two episodes generalizes well to the other.

\begin{table}[t!]
\centering
%\tiny
%\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{crrrrrrrrrr}
%\cline{2-11}
\toprule
\multirow{2}{*}{\textbf{Alg.}} & \multicolumn{9}{c}{\textbf{Episodes}} & \multirow{2}{*}{\textbf{Avg.}} \\
\cmidrule{2-10}
& \multicolumn{1}{c}{\textbf{1}}   & \multicolumn{1}{c}{\textbf{2}}   & \multicolumn{1}{c}{\textbf{3}}  & \multicolumn{1}{c}{\textbf{4}} & \multicolumn{1}{c}{\textbf{5}}  & \multicolumn{1}{c}{\textbf{6}}  & \multicolumn{1}{c}{\textbf{7}} & \multicolumn{1}{c}{\textbf{8}}  & \multicolumn{1}{c}{\textbf{9}} &    \\
\midrule
\multicolumn{1}{l}{\textbf{NB}}   &   73 & 88  &  91 & 85   &  94 & 81  &  41 & 74   & 94    &   80 \\
\multicolumn{1}{l}{\textbf{BERT}}   &  \textbf{89}  & \textbf{92}  & \textbf{96}  &  \textbf{88} & \textbf{97}   & \textbf{81} & \textbf{66}   &  \textbf{83}   &  \textbf{96} &  \textbf{87}  \\
\bottomrule
\end{tabular}
\caption{Valence accuracy in \% of Na\"ive Bayes (NB) and BERT for story emotion classification.}
\label{tab:valence}
\end{table}

Table \ref{tab:arousal} shows the accuracy of arousal classification of both NB and BERT. The best accuracy for a given episode is highlighted in bold. Again BERT outperforms NB in all the episodes, having an average accuracy 5\% higher. In contrast with the valence results, here there is no episode in which the BERT model substantially outperforms NB.

\begin{table}[h]
\centering
%\tiny
%\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{crrrrrrrrrr}
%\cline{2-11}
\toprule
\multirow{2}{*}{\textbf{Alg.}} & \multicolumn{9}{c}{\textbf{Episodes}} & \multirow{2}{*}{\textbf{Avg.}} \\
\cmidrule{2-10}
& \multicolumn{1}{c}{\textbf{1}}   & \multicolumn{1}{c}{\textbf{2}}   & \multicolumn{1}{c}{\textbf{3}}  & \multicolumn{1}{c}{\textbf{4}} & \multicolumn{1}{c}{\textbf{5}}  & \multicolumn{1}{c}{\textbf{6}}  & \multicolumn{1}{c}{\textbf{7}} & \multicolumn{1}{c}{\textbf{8}}  & \multicolumn{1}{c}{\textbf{9}} &    \\
\midrule
\multicolumn{1}{l}{\textbf{NB}}   &   82 & 88  &  75 & 79   &  82 & 76  &  98 & 86   & 84    &   83 \\
\multicolumn{1}{l}{\textbf{BERT}}   &  \textbf{86}  & \textbf{90}  & \textbf{77}  &  \textbf{86} & \textbf{89}   & \textbf{88} & \textbf{99}   &  \textbf{90}   &  \textbf{88} &  \textbf{88}  \\
\bottomrule
\end{tabular}
\caption{Arousal accuracy in \% of Na\"ive Bayes (NB) and BERT for story emotion classification.}
\label{tab:arousal}
\end{table}

\subsubsection{Music Emotion}

The music emotion classifier is a pair of GPT-2 models, one for valence and one for arousal. First, a GPT-2 LM is pre-trained with the new ADL Piano MIDI dataset. Each piece $p$ of this dataset was augmented by (a) transposing $p$ to every key, (b) increasing and decreasing $p$'s tempo by 10\% and (c) increasing and decreasing the velocity of all notes in $p$ by 10\% \cite{oore2017learning}. Thus, each piece generated $12 \cdot 3 \cdot 3 = 108$ different examples. The pre-trained GPT-2 LM has 4 layers (transformer blocks), context size of 1024 tokens, 512 embedding units, 1024 hidden units, and 8 attention heads. The pre-trained GPT-2 LM was then fine-tuned  independently using the VGMIDI dataset, for valence and for arousal. Similarly to BERT, fine-tuning a GPT-2 architecture consists of adding an extra classification head on top of the pre-trained model and training all parameters end-to-end. Similar to the story emotion classifiers, the GPT-2 classifiers are fine-tuned for 10 epochs using an Adam optimizer with learning rate 3e-5. Different from the story emotion classifiers, each training step was performed over mini-batches of size 16 (due to GPU memory constrains) and dropout of 0.25. The VGMIDI dataset is defined with a train and test splits of 160 and 40 pieces, respectively. The dataset was augmented by slicing each piece into 2, 4, 8 and 16 parts of equal length and emotion. Thus, each part of each slicing generated one extra example. This augmentation is intended to help the classifier generalize for pieces with different lengths.

The fine-tuned GPT-2 classifiers are compared with LSTM models that were also pre-trained with the ADL Piano Midi dataset and fine-tuned with the VGMIDI dataset. LSTMs were chosen because they are currently the state-of-the-art model in the VGMIDI dataset~\cite{ferreira_2019}. The LSTMs have same size as the GPT-2 models (4 hidden layers, 512 embedding units, 1024 hidden units) and were pre-trained and fine-tuned with the same hyper-parameters. Table \ref{tab:sent_accuracy} shows the accuracy of both models for valence and arousal. The performance of these models without pre-training (i.e., trained only on the VGMIDI dataset) are also reported. These are the baseline versions of the models.

\begin{table}[t!]
    \centering
    % \setlength{\tabcolsep}{4pt}
    \begin{tabular}{ccc}
    \toprule
    \textbf{Algorithm} & \textbf{Valence} & \textbf{Arousal} \\
    \midrule
    Baseline LSTM & 69 & 67 \\
    Fine-tuned LSTM & 74 & 79 \\
    Baseline GPT-2 & 70 & 76 \\
    Fine-tuned GPT-2 & \textbf{80} & \textbf{82} \\
    \bottomrule
    \end{tabular}
    \caption{Accuracy in \% of both the GPT-2 and LSTM models for music emotion classification. }
    \label{tab:sent_accuracy}
\end{table}

Results show that using transfer learning can substantially boost the performance of both models. The fine-tuned GPT-2 is 10\% more accurate in terms of valence and 8\% in terms of arousal. The fine-tuned LSTM is 5\% more accurate in terms of valence and 12\% in terms of arousal. Finally, the fine-tuned GPT-2 outperformed the fine-tuned LSTM by 6\% and 3\% in terms of valence and arousal, respectively.

\begin{table*}[!t]
\centering
%\tiny
%\small
\setlength{\tabcolsep}{4pt}
\begin{tabular}{crrrrrrrrrrrrrrrrrrrrrrr}
%\cline{2-11}
\toprule
\multirow{3}{*}{\textbf{Method}} & \multicolumn{20}{c}{\textbf{Episodes}} & \multicolumn{3}{c}{\multirow{3}{*}{\textbf{Average}}} \\
\cmidrule{2-21}
% & \multicolumn{4}{c}{\textbf{1}} & \multicolumn{4}{c}{\textbf{2}}   & \multicolumn{4}{c}{\textbf{3}} & \multicolumn{4}{c}{\textbf{4}} & \multicolumn{4}{c}{\textbf{5}} &    \\
& \multicolumn{2}{c}{\textbf{e1-p1}} & \multicolumn{2}{c}{\textbf{e1-p2}} & \multicolumn{2}{c}{\textbf{e2-p1}} & \multicolumn{2}{c}{\textbf{e2-p2}} & \multicolumn{2}{c}{\textbf{e3-p1}} & \multicolumn{2}{c}{\textbf{e3-p2}} & \multicolumn{2}{c}{\textbf{e4-p1}} & \multicolumn{2}{c}{\textbf{e4-p2}} & \multicolumn{2}{c}{\textbf{e5-p1}} & \multicolumn{2}{c}{\textbf{e5-p2}} \\
& \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{v} & \textbf{a} & \textbf{va}\\
\cmidrule{2-21}
\multicolumn{1}{l}{\textbf{Baseline}} & 56 & \textbf{65} & 39 & 56 & 39 & 62 & 39 & \textbf{79} & \textbf{48} & \textbf{60} & \textbf{67} & \textbf{53} & \textbf{58} & 70 & \textbf{63} & \textbf{75} & 25 & 36 & \textbf{72} & 58 & \textbf{51} & \textbf{32} & \textbf{34}\\
\multicolumn{1}{l}{\textbf{Composer}} & \textbf{62} & 60 & \textbf{44} & \textbf{65} & \textbf{82} & \textbf{68} & \textbf{53} & 68 & 24 & 55 & 46 & 43 & 25 & \textbf{87} & 37 & 55 & \textbf{81} & \textbf{86} & 51 & \textbf{67} & \textbf{51} & 30 & \textbf{34}\\
\bottomrule
\end{tabular}
\caption{The percentage of participants  that  correctly  identified  the valence and arousal (v and a, respectively) intended by the methods for the pieces parts (p1 and p2).
%The table also reports the average accuracy for all generated pieces in terms of valence (v), arousal (a), and jointly for valence and arousal (va).
}
\label{tab:user_study}
\end{table*}

\subsection{User Study}

Composer's performance in generating music that matches the emotions of a story is accessed with a user study. Composer is applied to generate a piece for a snippet composed of 8 contiguous sentences of each of the first 5 episodes of the CotW dataset. Each snippet has one emotion transition that happens in between sentences. The sentences are 5.18 seconds long on average. To test Composer's ability to generate music pieces with emotion changes, human subjects were asked to listen to the 5 generated pieces and evaluate the transitions of emotion in each generated piece.\footnote{Generated pieces can be downloaded from the following link: \url{https://github.com/lucasnfe/bardo-composer}} The user study was performed via Amazon Mechanical Turk and had an expected completion time of approximately 10 minutes. A reward of USD \$1 was given to each participant who completed the study.

In the first section of the study, the participants were presented an illustrated description of the valence-arousal model of emotion and listened to 4 examples of pieces from the VGMIDI dataset labeled with the valence-arousal model. Each piece had a different emotion: low valence and arousal, low valence and high arousal, high valence and low arousal, high valence and arousal. In the second section of the study, participants were asked to listen to the 5 generated pieces (one per episode). After listening to each piece, participants had to answer 2 questions: (a) ``What emotion do you perceive in the 1st part of the piece?'' and (b) ``What emotion do you perceive in the 2nd part of the piece?'' To answer these two questions, participants selected one of the four emotions: low valence and arousal, low valence and high arousal, high valence and low arousal, high valence and arousal. Subjects were allowed to play the pieces as many times as they wanted before answering the questions.
The final section of the study was a demographics questionnaire including ethnicity, first language, age, gender, and experience as a musician. To answer the experience as a musician, the participants used a 1-to-5 Likert scale where 1 means ``I've never studied music theory or practice'' and 5 means ``I have an undergraduate degree in music''.

Composer is compared with a baseline method that selects a random piece from the VGMIDI dataset whenever there is a transition of emotion. The selected piece has the same emotion of the sentence (as given by the story emotion classifier). A between-subject strategy has been used to compare these two methods, where Group $A$ of 58 participants evaluated the 5 pieces generated by Composer and another Group $B$ of 58 participants evaluated the 5 pieces from the baseline. This strategy was used to avoid possible learning effects where subjects could learn emotion transitions from one method and apply the same evaluation directly to the other method. The average age of groups $A$ and $B$ are 34.96 and 36.98 years, respectively. In Group $A$, 69.5\% of participants are male and 30.5\% are female. In Group $B$, 67.2\% are male and 32.8\% are female. The average musicianship of the groups $A$ and $B$ are 2.77 and 2.46, respectively.

Table \ref{tab:user_study} shows the results of the user study. The two parts (p1 and p2 in the table) of each episode are considered independent pieces. The table presents the percentage of participants that correctly identified the pieces' valence and arousal (``v'' and ``a'' in the table, respectively), as intended by the methods. This percentage is refeered to as the approach's accuracy. For example, 87\% of the participants correctly identified the arousal value that Composer intended the generated piece for part p1 of episode 4 (e4-p1) to have. The approaches' average accuracy is also presented across all pieces (``Average'' in the table) in terms of valence, arousal, and jointly for valence and arousal (``va'' in the table). The ``va''-value of 34 for Composer means that 34\% of the participants correctly identified the system's intended values for valence and arousal across all pieces generated.

Composer outperformed the Baseline in e1-p2, e2-p1, and e5-p1. Baseline outperformed Composer e3-p1, e3-p2 and e4-p2. In the other four parts, one method performed better for valence whereas the other method performance better for arousal. Overall, the average results show that both systems performed very similarly. Both of them had an average accuracy on the combined dimensions equal to 34\%. The difference between these two methods and a system that selects pieces at random (expected accuracy of 25\%) is significant according to a Binomial test ($p = 0.02$).
These results show that the  participants  were  able  to  identify  the  emotions  in  the generated pieces as accurately as they were able to identify the emotions in human-composed pieces. This is an important result towards the development of a fully automated system for music composition for story-based tabletop games.

\section{Conclusions}

This chapter presented Bardo Composer, a system that automatically composes music for tabletop role-playing games. The system processes sequences from speech and generates pieces one sentence after the other. The emotion of the sentence is classified using a fine-tuned BERT. This emotion is given as input to a Stochastic Bi-Objective Beam Search algorithm that tries to generate a piece that matches the emotion. Composer was evaluated with a user study and results showed that human subjects correctly identified the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces composed by humans.
