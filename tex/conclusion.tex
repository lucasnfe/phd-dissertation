This dissertation presented three search-based approaches to control music language models to generate symbolic music with a target emotion. The first approach, described in Chapter \ref{ch:ismir19}, consists of a genetic algorithm to optimize an LSTM language model towards generating negative or positive pieces \cite{ferreira_2019}. According to human evaluators, this approach showed to be successful in generating positive pieces, but negative pieces were considered slightly ambiguous. The second approach is called SBBS and is described in Chapter \ref{ch:aiide20}. It consists of controlling a stochastic beam search algorithm with emotion discriminators that steer the distribution of a language model towards a given emotion \cite{ferreira2020computer}. A listening test showed that human subjects could correctly identify the emotion of the pieces generated by SBBS as accurately as they were able to identify the emotion of pieces written by humans. The third approach, presented in Chapter \ref{ch:ismir21}, is a MCTS algorithm that uses PUCT with an emotion discriminator to search for music pieces with a target emotion. According to a listening test, MCTS outperformed SBBS in terms of quality and has slightly better accuracy when controlling emotions.

A dataset of symbolic music called VGMIDI has been created to support these three approaches. VGMIDI has 200 pieces labeled according to the circumplex model of emotion \cite{russell1980circumplex}, and an additional 3,640 unlabelled pieces. All of them are piano arrangements of video game soundtracks. The labeling process was performed by 30 annotators with a custom web tool designed as part of this dissertation. Moreover, this dissertation also presented Bardo Composer, a system to generate music for tabletop role-playing games. Bardo Composer uses a speech recognition system to translate player speeches into captions, which Bardo Composer classifies according to a model of emotion. Bardo Composer then uses SBBS with a neural music emotion classifier to generate pieces conveying the emotion detected in the captions.

The contributions of this dissertation showed that searching over the space defined by music language models with the guidance of music emotion classifiers has strong potential in generating music with controllable emotion. As discussed in Chapter \ref{ch:future}, there are several possibilities for future work, from increasing the VGMIDI dataset to developing new generative models and applying Bardo Composer to other media (e.g., video games and films). Hopefully, the contributions of this dissertation will inspire others to explore some of this future work and other ideas that will enable generative models to compose music with human level quality.
