Deep generative models are currently the leading methods for AMC \cite{yang2019deep}, however, one of the major problems of these methods is to control the generation process towards a given compositional goal. For example, controlling a model trained on classical piano pieces to compose a tense piece for a horror scene of a movie. Controlling emotions in AMC has been actively investigated in the field of \textit{affective algorithmic composition} \cite{williams2015investigating}. Therefore, this chapter presents a literature review of this field, including models to represent emotion and methods to control emotion in symbolic AMC systems. Given the similarity between text and music generation tasks, this chapter also covers NLP methods to control neural natural language models.
% Besides controlling emotions, the AMC research community has also been investigating how to control structural attributes (e.g. chord progression, melody contour, orchestration) of music with deep generative models. These methods are also discussed in this chapter.
% Once again, it is important to highlight that only  music generation methods are discussed in this chapter.

\section{Affective Algorithmic Composition}

Affective Algorithmic Composition (AAC) is a research field concerned about AMC methods that compose music to make a listener perceive or feel a given target emotion \cite{williams2015investigating}, with applications ranging from soundtrack generation \cite{williams2015dynamic} to sonification \cite{Chen2015} and music therapy \cite{miranda2011brain}. The AAC literature examined various methods, such as expert systems, evolutionary algorithms, Markov chains, and neural networks. These methods have used different \textit{models of emotion}, but most of them are concerned with controlling \textit{perceived emotions}, instead of \textit{felt emotions}. Therefore, they are normally evaluated with a qualitative analysis of generated samples or listening tests with human subjects. This section introduces the most common models of emotion used in the AAC literature and the different approaches to control emotion in AMC. It also highlights the different methodology used in the AAC literature to conduct the qualitative analysis and the listening tests.

\subsection{Models of Emotion}

The study of affective phenomena has a very dense literature with multiple alternative theories \cite{ekkekakis2013measurement}. This section doest not aim to discuss all of them but to provide a definition of emotion helpful for designing neural generative models that can compose music with controllable emotion.  According to Williams et al. \cite{williams2015investigating}, the literature concerning the affective response to musical stimuli defines emotion as a short-lived episode, usually evoked by an identifiable stimulus event that can further influence or direct perception and action. Williams et al. \cite{williams2015investigating} also differentiate emotion from affect and mood, which are longer experiences commonly caused by emotions.

There are two main types of models used to represent emotions: \emph{categorical} and \emph{dimensional} \cite{eerola2011comparison}. Categorical models use discrete labels to classify emotions. For example, Ekman's model \cite{ekman1992argument} divides human emotions into six basic categories: anger, disgust, fear, happiness, sadness, and surprise. This model builds on the assumption that an independent neural system subserves every discrete basic emotion \cite{eerola2011comparison}. Therefore, any other secondary emotion (e.g. rage, frustration, grief) can be derived from the basic ones. Parrott \cite{parrott2001emotions} proposed a similar model, but deriving a hierarchical structure with six basic emotions: joy, surprise, anger, sadness, and fear. Basic emotions are expanded into secondary emotions (e.g. passion, pleasure, envy), which in turn derive ternary emotions (e.g. compassion, frustration, guilt).

Some emotions are more present in the music domain and are evoked more easily than others. For example, it is more common for a person to feel happiness instead of disgust while listening to music. The Geneva Emotion Music Scale (GEMS) \cite{zentner2008emotions} is a categorical model specifically created to capture the emotions that are evoked by music. GEMS divides the space of musical emotions into nine categories: wonder, transcendence, tenderness, nostalgia, peacefulness, energy, joyful activation tension, and sadness. These nine emotions group a total of 45 specific labels.

Dimensional models represent emotions as a set of coordinates in a low-dimensional space. For example, Russell \cite{russell1980circumplex} proposed a general-purpose dimensional model called \textit{circumplex}, which describes emotions using two orthogonal dimensions: valence and arousal. Instead of an independent neural system for every basic emotion, the circumplex model that all emotions arise from two independent neurophysiological systems dedicated to the processing of valence (positive--negative) and arousal (mild--intense). Figure \ref{fig:circumplex} shows a graphical representation of the circumplex model. The \textit{vector model} is another two-dimensional model based on valence and arousal \cite{bradley1992remembering}. It represents emotions as vectors, where valence is a binary dimension (positive or negative) that defines the vector's direction and arousal is a continuous dimension that defines the vector's magnitude.

\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{imgs/related_work/circumplex.png}
\caption{The circumplex model of emotion. The horizontal and vertical axes represent valence and arousal,
respectively \cite{russell1980circumplex}.}
\label{fig:circumplex}
\end{figure}

Independent of the model, emotions can be also classified as \textit{perceived} or \textit{felt}.
A person \textit{perceives} an emotion when she objectively recognizes that emotion from her surroundings. For example, one can usually recognize others' emotions using expressed cues, including facial expression, tone of voice, and gestures. The same can happen when one listens to music, in that she recognizes the music as happy or sad using cues such as key, tempo, or volume. A person \textit{feels} an emotion when she actually experiences that emotion herself. For example, one typically experiences fear in response to someone else's anger. This example shows that a given perceived emotion can trigger a different felt emotion. Zentner, Grandjean, and Scherer \cite{zentner2008emotions} showed that emotions are less frequently felt in response to music than they are perceived as expressive properties of the music.

\subsection{Expert systems}

Expert systems are one of the most common methods of AAC \cite{williams2015investigating}. They encode knowledge from music composers to map musical features into a given emotion in a categorical or dimensional space. For example, Williams et al. \cite{williams2015dynamic} proposed a system to generate soundtracks for video games from a scene graph \footnote{A graph defining all the possible branching of scenes in the game.} annotated according to twelve categorical emotions derived from the circumplex model. A second-order Markov chain was used to learn melodies from a symbolic music dataset which are then transformed by an expert system to fit the annotated emotions in the graph. Each of the twelve emotions was mapped to a different configuration of five music parameters: rhythmic density, tempo, modality (major or minor), articulation, mean pitch range, and mean spectral range. These pre-defined configurations were used to transform the generated melodies. For example, a melody generated for a happy scene would be transformed to have high density, medium tempo, major mode, staccato articulation, medium mean pitch range, and high spectral range (clear timbre).
Williams et al. \cite{williams2015dynamic} evaluated their system by qualitatively examining a few examples of generated pieces.

TransProse \cite{davis2014generating} is an expert system that composes piano melodies for novels. It splits a given novel into sections and uses a lexicon-based approach to assign an emotion label to each section. TransProse composes a melody for each section by controlling the scale, tempo, octave, and notes of the melodies with pre-defined rules based on music theory. For example, the scale of a melody is determined by the sentiment of the section: positive sections are assigned a major scale, and negative sections are assigned a minor scale. TransProse was evaluated with a qualitatively analysis of nine music pieces generated by the system for nine respective novels.

Scirea et al. \cite{scirea2017affective} presented a framework called MetaCompose designed to create background music for games in real-time. MetaCompose generates music by (i) randomly creating a chord sequence from a pre-defined chord progression graph, (ii) evolving a melody for this chord sequence using a genetic algorithm and (iii) producing an accompaniment, adding rhythm and arpeggio, for the melody/harmony combination. Finally, MetaCompose uses an expert system called \textit{Real-Time Affective Music Composer} to transform the final composition to match a given emotion in the circumplex model. This expert system controls four musical attributes: volume, timbre, rhythm, dissonance. For example, low arousal pieces were controlled to have lower volume. Scirea et al. \cite{scirea2017affective} evaluated each component of the MetaCompose with a pairwise listening test. The components of the system were systematically (one-by-one) switched off and replaced with random generation, generating different ``broken'' versions of the framework. These broken versions were paired with the complete framework and evaluated by human subjects according to four criteria: pleasantness, randomness, harmoniousness, and interestingness. For each criteria, the participants were asked to prefer one of two pieces, also having the options of \textit{neither} and \textit{both equally}.

\subsection{Evolutionary Algorithms}

To control emotions in music generated with EAs, one has to define a fitness function that guides individuals encoding symbolic music towards a given perceived emotion. It is very challenging to design a function that formally evaluates subjective aspects of music emotion. Thus, most EAs for AAC use interactive evaluation functions, where human subjects judge whether or not the generated pieces match a target emotion. The benefit of this approach is that when the population converges towards a target emotion, no further evaluation is needed to check if the generated pieces indeed match that emotion.
However, every time one wants to generate a new set of pieces, the slow interactive evolutionary process
has to be restarted.

Kim and Andr{\'e} \cite{kim2004composing} proposed a genetic algorithm to generate polyrhythms\footnote{A polyrhythm is the concurrent playing of two or more different rhythms.} for four percussion instruments. It starts with a random population of polyrhythms and evolves them towards a given target emotion (e.g. relaxing or disquieting). A polyrhythm is encoded with four 16-bit strings, one for each instrument. A single bit in the string represents a beat division where 1 means that a (non-pitched) note is played in that division and 0 means silence. The fitness of a polyrhythm is given by a human subject who listens to it and judges it as relaxing, neutral, or disquieting. The selection strategy keeps the four most relaxing and four most disquieting individuals for reproduction with one-point crossover and mutation. Results showed that the genetic algorithm generated relaxing polyrhythms after 20 generations while it took only
10 for it to generate disquieting ones.

Zhu et al. \cite{zhu2008emotional} presented an interactive genetic algorithm based on the KTH rule system \cite{friberg2006overview}. This system models performance principles within the realm of Western classical, jazz and popular music. These rules control different music performance parameters (e.g. phrasing, articulation, tonal tension) with weights called \textit{k values} that control the magnitude of each rule. Zhu et al. \cite{zhu2008emotional} encoded the individuals as a set of k values used to create MIDI performances of the given pieces according to the KTH rules. The genetic algorithm evolves a population to find optimal k values that yield performances that are either happy or sad. The fitness of the performances are given by human subjects with a seven-point Likert scale.

Nomura and Fukumoto \cite{nomura2018music} designed a distributed interactive genetic algorithm to generate four-bar piano melodies with controllable ``brightness''. Multiple human evaluators evolve independent populations of melodies in parallel. In some generations, the genetic algorithm exchanges individuals between the independent populations. With the exchange, evaluators are affected by each other and the solutions are expected to agree with everyone's evaluations. Each individual in a population represents a melody with sequence of sixteen pitch numbers (as defined by the MIDI protocol). Each element in the sequence is mapped into a quarter note with the pitch defined by the element. Evaluators give the fitness of an individual based on a seven-point Likert scale, where 1 means ``extremely dark'', 4 means ``neither'', and 7 means ``extremely bright''. An experiment with ten parallel evaluators showed that after seventeen generations, the independent populations converged to similar melodies.

\subsection{Markov Chains}

AAC systems based on Markov chains are typically hybrid systems where Markov chains compose an underlying piece of music which is transformed by an expert system. The work of Williams et al. \cite{williams2015dynamic} described earlier in this chapter is an example of such a system. Ramanto and Maulidevi \cite{ramanto2017markov} proposed a similar system in which the Markov chain is designed manually by the authors, instead of derived from a corpus. There are not a lot works training Markov chains directly from datasets of symbolic music labelled according to a model of emotion. One of the major examples is the work of Monteith et al. \cite{monteith2010automatic, chan2008automatic}, which use different Markov models to generate polyphonic music from a MIDI corpus labelled according to the Parrott model of emotion \cite{parrott2001emotions}. The corpus is composed by 45 movie soundtracks labelled by 6 researchers using the 6 basic Parrott emotions: joy, surprise, anger, sadness, and fear.

The system is divided into four main components: a rhythm generator, a pitch generator, a chord generator, and an accompaniment/instrumentation planner. First, the rhythm generator randomly selects a phrase from the subset of pieces with the target emotion and transforms its rhythm by randomly changing a number of measures. Second, the pitch generator assigns pitch values to the notes in the rhythm by sampling from a n-gram (i.e. Markov chain) trained with the pieces from the target emotion. Third, the chord generator generates an underlying harmony for the provided melody with a Hidden Markov Model in which pitches are considered observed events and the chord progression underlying state sequence. Finally, the instrumentation planner probabilistically selects the instruments for melody and harmonic accompaniment based on the frequency of various melody and harmony instruments in the corpus. The system uses single-layer feedforward networks, trained with the same MIDI corpus, to classify the outputs of the rhythm and melody generators as having the target emotion (positive) or not (negative). The system only accepts rhythms and melodies that are classified as positive by the neural networks. Monteith et al. \cite{monteith2010automatic} evaluated their system with a listening test in which thirteen human subjects listened to twelve generated pieces. Each subject judged a piece by selecting the emotion that they perceived in the piece. Moreover, the subject used two 10-point Likert scales to evaluate how human-like, and unique the pieces are.

% Chung and Vercoe \cite{chung2006affective} proposed a method to derive a Markov chain from physiological, physical, and self-reported data from listeners. They first composed original music pieces with different styles: jazz, jazz-funk, rock, electronic and dance music. Four audio clips were assembled for each of the five musical styles, providing 20 clips in total, each with a different level of vertical layering\footnote{The amount of musical layers stacked in a piece.} and horizontal complexity \footnote{The rhythmic complexity of a piece}.

\subsection{Deep Learning}

Deep NNs started being used for AAC very recently. Indeed, this dissertation is one of the first works in this area. One of the main challenges of using deep NNs for AAC is the lack of labelled data. It is considerably expensive to create a large dataset of symbolic music labelled according to a model of emotion. However, a few works have been proposed. Madhok et al. \cite{madhok2018sentimozart} introduced one of the few works prior to this dissertation with the framework SentiMozart, which generates piano pieces that match the emotion of a given facial expression. SentiMozart uses a convolutional NN to classify the emotion of a facial expression and an LSTM to generate a music piece corresponding to the identified emotion. The convolutional NN was trained with the FER-2013 \cite{goodfellow2013challenges} dataset, which has 35,887 images of facial expressions labelled with the following emotions: angry, disgust, fear, happy, sad, surprise, and neutral. To train the LSTM, Madhok et al. \cite{madhok2018sentimozart} created a dataset with 200 MIDI piano pieces, each labelled by 15 annotators according to the classes happy, sad, and neutral. An LSTM was trained independently for each of these three emotions. To unify the models of emotion between the two datasets, Madhok et al.\cite{madhok2018sentimozart} merged the categories sad, fear, angry, and disgust from the FER-2013 dataset into the sad emotion. The categories happy and surprise were mapped into the happy emotion. Thus, the output of the convolutional NN was one of these three emotions and was used to select the respective LSTM that, in turn, composes the music with the given emotion. SentiMozart was evaluated with a listening test where thirty human subjects judged 30 randomly chosen images (10 of each class) and their corresponding generated music. The participants used a ten-point Likert scale in which 0 means sad, 5 means neutral, and 10 means happy.

Tan and Antony \cite{tan2020automated} proposed a deep generative NN for a similar problem: generating music that matches an emotion expressed by visual artworks (e.g. paintings, illustrations, collages). They paired images of paintings with MIDI piano pieces labeled with the same emotion and used an encoder-decoder network as a generative model. The encoder is a pre-trained convolutional NN called ResNet \cite{he2016deep}. Tan and Antony \cite{tan2020automated} compared two decoders: a LSTM and a transformer decoder.
The music pieces were encoded as sequences of tokens with the MIDI-based method proposed by Oore et al. \cite{oore2017learning}. Tan and Antony \cite{tan2020automated} trained their model by pairing emotion-labeled images from You et al. \cite{you2016building} (17,349 images) with emotion-labeled music pieces from the multi-modal MIREX-like emotion dataset (196 MIDI files) \cite{panda2013multi}. Since these two datasets use different categorical models of emotion, Tan and Antony \cite{tan2020automated} mapped the emotion labels of the music dataset to the labels of the image dataset. The trained model was evaluated with a listening test and a machine classification test. Six human subjects performed the listening test. They were asked to evaluate the sentiment of the music pieces and the image ons a ten-point Likert scale without knowing which piece of music was related to which image. The machine evaluation test consisted of training an emotional correspondence classifier to predict if a given pair of images and pieces express the same sentiment.

Zhao et al. \cite{zhao2019emotional} presented a conditional Biaxial LSTM \cite{johnson2017generating} to generate piano pieces with controllable emotion. They labelled the \textit{Piano midi.de} dataset according to a categorical model of emotion: happy, sad, peaceful, and tensional. They used a piano-roll representation for the input music pieces, which are processed by their model together with an emotion signal associated with each time-step of the input. This signal is encoded by an extra embedding layer and then added to the embedding of the input. Zhao et al. \cite{zhao2019emotional} evaluated the qualify of the generated pieces with four metrics of music structure: polyphony, scale consistency, 3-tone repetitions, and tone span. Moreover, they performed a listening test with 30 human subjects to evaluate if the subjects agree with the emotions intended by the model.

Music FaderNets \cite{tan2020music} is a framework based on a Gaussian Mixture Variational Autoencoder (GM-VAE) that can learn high-level music feature representations (such as emotion) by modelling corresponding low-level structural music features. Based on the work Yang et al. \cite{yang2019deep}, Music FaderNets learns low-level features of rhythm $z_r$, note density $z_d$, and key $z_k$ with separated RNN encoders $e_r$, $e_d$, and $e_k$,  respectively. High-level features are then inferred from the low-level representations ($z_r$, $z_d$, and $z_k$) via semisupervised clustering. Music Fadernets was trained to reconstruct pieces encoded as sequence of tokens \cite{oore2017learning} extracted from the Yamaha Piano-e-Competition dataset \cite{yamahaEPiano} and the VGMIDI dataset\footnote{The VGMIDI dataset is contribution of this dissertation described in Chaper \ref{ch:ismir19}} \cite{ferreira_2019}. Results showed that Music Fadernets successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes rhythm and note density.

% \section{Conditional Generative Models}

% Besides controlling emotion, the AMC community has also been investigating how to control general structural features in music. These methods typically are unsupervised approaches to learn disentangled representations of specific music features such as melody and rhythm. Such disentangled representations allows one to directly manipulate the output of the model. These approaches are related to this dissertation because one could use them to learn disentangled mappings between music structure and emotions. For example, EC2-VAE \cite{yang2019deep} is a VAE that learns disentangled representations of symbolic melodies, allowing the generation of melodies with controlled pitch contour and rhythm patterns. The encoder is a bidirectional GRU $E$ that maps the input melodies into a latent representation $z$. The decoder splits the latent representation $z$ to create two independent representations $z_p$ and $z_r$ for the pitch and the rhythm features of the melody, respectively. The rhythm representation $z_r$ is fed into a bidirectional GRU $D_r$ to reconstruct the rhythm of the melody. The output of $D_r$ is concatenated with $z_p$ and fed into another GRU $D_m$ to reconstruct the entire melody. Moreover, the EC2-VAE uses chords as a condition for both the encoder and decoder. EC2-VAE was trained with the Nottingham Dataset \cite{} to reconstruct entire melodies. The two independent decoder modules allows it to learn disentangled representations of pith and rhythm. Therefore, one can swap or interpolate the latent representations of different source melodies to compose new melodies that combine pitch and rhythm features from the source material.
%
% Music Sketchnet \cite{chen2020music} is a neural network \textit{music inpainting}\footnote{Music inpainting is the task of filling in missing or lost information in a piece of music.} framework that allows users to specify partial musical ideas guiding automatic music generation. Music Sketchnet is composed of three NN modules: SketchVAE, SketchInpainter, and SketchConnector. Similar to EC2-VAE \cite{yang2019deep}, SketchVAE \cite{chen2020music} is a VAE that creates independent representations of rhythm and pitch contour. SketchInpainter combines GRUs to perform the element-level inpainting prediction from the latent variables given by SketchVAE. SketchConnector uses a transformer NN to combined sketches of pitch and rhythm from users with the prediction from SketchInpainter and finalize the generation. Sketchnet was trained to reconstruct melodies from the Irish and Scottish monophonic music dataset \cite{}.

% Our work differs from these generative models because, instead of controlling structural music features, we control emotion which is a perceived feature that emerges from the music structure.

\section{Controllable Neural Language Models}

Considering the similarities between music and text neural LMs, this dissertation is also related to methods that control neural LMs to generate text with given characteristics (e.g. topic, style, sentiment). For example, Radford et al. \cite{radford_2017} showed that an LSTM can learn an excellent representation of sentiment (positive-negative) on text, despite being trained only to predict the next character in the Amazon reviews dataset \cite{He2016}. When fine-tuned with an extra linear layer, this LSTM achieved state-of-the-art sentiment analysis on the stanford sentiment treebank dataset \cite{socher2013recursive} and matched the performance of previous supervised systems using 30-100x fewer labeled examples. Radford et al. \cite{radford_2017} observed that this LSTM stored almost all of the sentiment signal in a distinct \textit{sentiment neuron}, which could be used to control the LSTM to generate sentences with a given sentiment.

CTRL \cite{keskar2019ctrl} is a transformer LM trained to generate text conditioned on special tokens that inform the LM about the characteristics of the text to be generated (e.g., style). These control codes are derived automatically from the origin of the text (e.g., Wikipedia), which means no manual annotation had to be performed. During training, every example is fed as input together with its control codes. CTRL can be used to generate text with a particular style, for example, by feeding the information that represents that style. This dissertation differs from CTRL because we control the LM with a search procedure and not with an extra input to the LM. Conditioning the LM requires a large amount of labeled data, which is expensive in the affective music domain.

Holtzman et al. \cite{holtzman2018learning} proposed a decoding strategy with a set of discriminator NNs that steer the probability distribution of a LM during generation time. At each decoding step, the probabilities of the next word given by the LM are multiplied by weighted scores of four different discriminators: repetition, entailment, relevance, and lexical diversity. The discriminators are trained independently with a dataset of pairs $(x, y_g)$, where $x$ is a prior context sentence, and $y_g$ is the completion of that sentence. At each independent training phase, the loss function measures the difference between the scores assigned to the truth continuation $y_g$ and the scores assigned to the continuation generated by the LM $\hat{y}$. Once all the discriminators are trained, Holtzman et al. \cite{holtzman2018learning} optimize the weight coefficients that will be used when decoding the LM.

The Plug and Play LM \cite{dathathri2019plug} combines a pre-trained LM $L$ with an attribute (e.g. sentiment) classifier $C$ to guide text generation by fine-tuning the hidden layers of the LM in decoding time. At each generative time-step, the hidden layers of LM are shifted in the direction of the sum of two gradients: one towards the higher log-likelihood of the target attribute as given by $C$ and the other towards higher log-likelihood of the unmodified LM $L$. The shifting process occurs in three steps: (1) a first forward pass using $C$ to compute the likelihood of the target attribute, (2) a backward pass to update the LM hidden states with gradients from the attribute classifier $C$, and (3) a final forward pass to update the distribution over the vocabulary from the updated LM hidden layers. With this approach, multiple attribute classifiers can be combined during the generation with customized weights.

% Vijayakumar et al. \cite{vijayakumar2018diverse} and Kool et al. \cite{Kool2019SBS} proposed variations of beam search to solve the problem of generating repetitive sentences. Vijayakumar et al. \cite{vijayakumar2018diverse} decodes diverse lists by dividing candidate solutions into groups and enforcing diversity between groups. Kool et al. \cite{Kool2019SBS} applyes the ``Gumbel-Top-k'' trick to sample without replacement with beam search, mitigating the repetition problem. Our work differs from both these works because our variation of Beam search optimizes for two independent objectives.

% Ziegler et al. \cite{ziegler2019fine} used reinforcement learning to fine-tune a transformer LM for generating text with a given sentiment by using a reward model trained on human preferences on text continuations. They fine-tuned the model for two different tasks: generating text with sentiment and text summarization.
Given a music dataset labeled according to emotion, one could apply the NLP methods discussed in this section to control music LMs to generate pieces with a target emotion. Indeed, the first two contributions of this dissertation are inspired by such methods. The next chapter presents a method based on Radford et al. \cite{radford_2017} to control the sentiment of piano pieces generated by an LSTM. Chapter \ref{ch:aiide20} discusses a variation of beam-search inspired by Holtzman et al. \cite{holtzman2018learning} to generate music with controllable emotion for tabletop role-playing games.
