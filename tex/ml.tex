\textit{Deep Learning} is a class of \textit{Machine Learning} (ML) algorithms based on \textit{Neural Networks} (NNs) with multiple layers that progressively extract higher-level features from raw data (e.g., text, images, audio, and video) \cite{goodfellow2016deep}. As ML algorithms, deep neural networks are used to learn different tasks from examples without being explicitly programmed to do so, including \textit{supervised learning} tasks, \textit{unsupervised learning} tasks, and \textit{reinforcement learning} tasks. In a \textit{supervised learning} task, pairs $(X,Y)$ of inputs $X$ and target classes (also called labels) $Y$ are provided by a dataset as training examples. The NN then is \textit{trained} to learn a function that maps the input examples into the target classes. The learned function is typically used to perform predictions (e.g., classification) on examples that the NN has not seen during training. Supervised learning is typically divided into \textit{binary} and \textit{multiclass} problems. In binary problems, a given input $x$ can have one of two possible values $y \in [0, 1]$. In multiclass problems, the label $y \in [0, 1, \cdots, L]$ can take one of $L > 2$ values. Classic examples of binary and multiclass problems are email spam detection (spam or not) and handwritten digits classification \cite{lecun1998gradient}, respectively. In unsupervised learning tasks, only the inputs $X$ are given in the dataset, and the NN is trained to learn internal patterns in the data. These learned patterns can be used for different purposes such as clustering, transfer learning, and generative modeling \cite{bengio2012unsupervised}. In reinforcement learning, the NN is trained to learn an agent that can take optimal actions in an environment according to a \textit{reward function}.

Modern neural AMC systems are typically designed in an unsupervised learning setting, where a NN has to learn relationships between different music structures (e.g., notes, chords, and melodies) represented in symbolic format. Formally, these NNs are \textit{generative models}, i.e. a model that captures a probability distribution $P(X)$ from a given dataset $X$. Inspired by the great results that deep learning has achieved in NLP, generative models for AMC are typically designed as neural \textit{language models} (LM). In NLP, a LM is a conditional probability $L = P(x_t|x_1, x_2, \cdots, x_{t-1})$ of the next token $x_t$ given a prefix with the $t-1$ previous tokens $\{x_1, x_2, \cdots, x_{t-1}\}$ of a sentence. One can train a NN to learn $L$ by processing input sequences of tokens $\{x_1, x_2, \cdots, x_{t-1}\}$ to predict the next token $x_{t+1}$ from the current token $x_t$. A NN trained this way can generate new sentences by sampling tokens from $L$ or searching for sequences over the space defined by $L$.

Considering that music is a sequence of musical tokens (e.g., notes, chords, and sections), one can train a neural LM $L$ to compose music by (a) creating a dataset of symbolic music, (b) designing a NN to learn $L$, and (c) sampling or searching tokens with $L$. The remainder of this chapter discusses different approaches for these three steps.

% Representation and Datasets
\section{Symbolic Music Representation}

Symbolic music representation refers to using high-level symbols such as tokens, events, or matrices as a representation for music modeling. The advantage of symbolic music representation over audio music representation is that the former incorporates higher-level features (e.g., structure, harmony, and rhythm) directly within the representation itself, without the need for further preprocessing. There are many formats to represent symbolic music in computers, but the most common ones are MIDI and piano roll.

\subsection{MIDI}
MIDI is a standard protocol for interoperability between various electronic instruments, devices, and software \cite{briot2017deep}. A MIDI file represents a music piece as a series of messages that specify real-time note performance data and control data. The two most important MIDI messages for music LMs are the following:

\begin{itemize}
  \item NOTE\_ON: this message is sent when a note starts, and it has three parameters:
  \begin{itemize}
      \item \textit{Channel number}: indicates the instrument track with an integer $0 \leq i \leq 15$
      \item \textit{Note number}: indicates the note pitch with an integer $0 \leq p \leq 127$
      \item \textit{Note velocity}: indicates how loud the note is played with an integer
      $0 \leq v \leq 127$
  \end{itemize}

  \item NOTE\_OFF: this message is sent when a note ends, and it has the same three parameters as the
  NOTE\_ON message. In this case, the velocity parameter indicates how fast the note is released.
\end{itemize}

Note events are organized into a stream format called \textit{track chunk}, which specifies the timing information of each note event with a delta time value. A delta time value represents the time of the note event either in relative metrical time (number of ticks from the beginning) or absolute time. In the relative metrical format, a reference called \textit{division} is defined in the file header to set the number of ticks per quarter note. Table \ref{tab:midi} shows an example of a MIDI \textit{track chunk} encoded in a readable format, where the time division has been set to 384 ticks per quarter note.

\begin{table}[h]
    \centering
    % \setlength{\tabcolsep}{4pt}
    \begin{tabular}{llllll}
        \textbf{Delta time} & \textbf{Event Type} & \textbf{Channel} & \textbf{Pitch} & \textbf{Velocity} \\
        96  & NOTE\_ON  & 0 & 60 & 90 \\
        192 & NOTE\_OFF & 0 & 60 & 0  \\
        192 & NOTE\_ON  & 0 & 62 & 90 \\
        288 & NOTE\_OFF & 0 & 62 & 0  \\
        288 & NOTE\_ON  & 0 & 64 & 90 \\
        384 & NOTE\_OFF & 0 & 64 & 0
    \end{tabular}
    \caption{Example of MIDI file encoded in a readable format \cite{briot2017deep}.}
    \label{tab:midi}
\end{table}

% To use MIDI formats with LMs, one has to create a vocabulary of symbols that represent
% note events and their attributes. This vocabulary is then used to encode the stream of events
% in a given set of MIDI files as a sequence of symbols.
% There are many different ways to do that with smaller or bigger vocabularies.
% The pros and cons of some of the common approaches are discussed later in this chapter.

\subsection{Piano Roll}

\textit{Piano roll} is another common format of symbolic music. It is inspired by classic automated pianos that play pieces without a human performer by reading music from a continuous roll of paper with perforations punched into it. Each perforation automatically triggers a note, where the perforation location defines the note pitch, and the perforation length defines the note duration. In a modern piano roll, music is divided into discrete time steps forming a grid where the x axis represents time and the y axis represents pitch. The values $0 \leq v \leq 127$ in the grid represent the velocity of the notes. Figure \ref{fig:piano_roll} shows an example of a modern piano roll.

\begin{figure}[!h]
 \centering
 \includegraphics[width=\columnwidth]{imgs/background/piano_roll.jpg}
 \caption{Example of music represented in the piano roll format.}
 \label{fig:piano_roll}
\end{figure}

% Using a piano roll representation with LMs also requires creating a
% vocabulary of symbols to represent notes and their attributes
% (duration and velocity). However, the sequence of symbols is defined
% by the time steps in the piano roll, instead of the stream of events
% in the MIDI representation.

The MIDI representation can be mapped to a piano roll by sampling time steps with a given frequency (e.g., every 16th note) from the MIDI events. Because of this property, most datasets of symbolic music organize pieces in a collection of MIDI files. For example, the \textit{MAESTRO} dataset \cite{hawthorne2018enabling} is composed of about 200 hours of virtuosic piano performances of classical music pieces captured from the International Piano-e-Competition \cite{yamahaEPiano} in MIDI format aligned with audio waveforms. The \textit{Lakh} \cite{raffel2016learning} dataset is a collection of 176,581 unique MIDI files from various music genres (mostly pop music) scraped from publicly available sources on the internet, where 45,129 of them have been matched and aligned to entries in the \textit{Million Song Dataset} \cite{bertin2011million}. \textit{Piano midi.de} is a dataset of classical piano pieces from a wide variety of composers recorded in MIDI with a digital piano. JSB Chorales \cite{boulanger2012modeling} contains the entire corpus of 382 four-part harmonized chorales by J. S. Bach.

MIDI and piano roll are the most common formats used to represent symbolic music. However, other formats have also been used in the AMC literature. For example, the ABC notation \cite{walshaw1993abc2mtex} is a text-based music notation system popular for transcribing, publishing, and sharing folk music. MusicXML \cite{good2001musicxml} is a markup language that has been designed to facilitate the sharing, exchange, and storage of scores by musical software systems.

To use any of these symbolic music representations with LMs, one has to define a vocabulary that encodes music data into a sequence of music symbols. %These symbols will be used to create sequences that represent music pieces.
For example, in a MIDI representation, one has to map the note events into tokens and use the delta-time information from the \textit{track chunks} to define the order of the tokens. In a piano roll representation, one has to map the vertical axis (pitch) into tokens and process the piano roll grid either horizontally or vertically to define the order of the tokens. To be processed by NNs, each token in the vocabulary has to be mapped into a vector. Traditionally, these tokens are mapped using \textit{one-hot} encoding, where each token is given an index $i$ and is represented by a vector $v = [v_1, v_2, \cdots, v_n]$, where only $v_i = 1$ and all the other dimensions $v_{j \neq i} = 0$. In the one-hot encoding, $n$ is the number of tokens in the vocabulary.
For example, considering a vocabulary $V = \{a, b, c, d, e\}$, the one-hot encoding of the token $c$ is $c = [0,0,1,0,0]$.

\section{Neural Networks}

% With a dataset of music represented in a symbolic format (MIDI, piano roll, etc.),
% one can train a ML model to learn a LM that can generate music.
% In early examples, LMs were typically implement with Markov chains and
% recently LMs are more commonly modeled with artificial neural networks.
Artificial Neural Networks, or simply Neural Networks (NNs), interconnect a number of simple processing units called \textit{neurons} to learn a function from training examples. These neurons are typically organized into layers. Neurons might be connected to several other neurons in the layer before it, from which it receives data, and several neurons in the layer after it, to which it sends data. NNs can be defined with different \textit{architectures}, i.e. with a different number of layers (\textit{depth}) and different layouts of neuron connections. The first layer of the network is called the \textit{input layer}, and the last one is called the \textit{output layer}. All the intermediate layers are called \textit{hidden layers}. Each neuron in the hidden or output layers takes as input a vector $x$ of incoming connections from the previous layer and assigns a weight vector $w$ to these connections. In its most basic form, the neuron first applies a linear transformation $z = w \dot x + b$ to the inputs $x$, where $b$ is an extra weight called \textit{bias} that is not tied to any neuron of the previous layer. The neuron then uses a nonlinear function called \textit{activation function} $f$ to map the linearly transformed inputs $z$ into an output $\hat{y}$. Figure \ref{fig:nn} shows a three-layer\footnote{Typically, the input layer is not considered when counting the depth of the NN.} NN called \textit{feedforward} network or \textit{multilayer perceptron} (MLP).

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\columnwidth]{imgs/background/ann.png}
\caption{Diagram of a feedforward neural network \cite{lecun2015deep}. The computations performed by the neurons $l$ (hidden layer H1), $k$ (hidden layer H2), and $l$ (output layer) are highlighted beside them. The bias terms have been removed for clarity.}
\label{fig:nn}
\end{figure}

The number of neurons per layer (i.e., the \textit{layer size}) and the layers' activation functions depend on the task being learned by the NN. In traditional supervised problems, the input layer size is defined by how the input examples are represented and the output layer size by the number of classes in the problem. For example, consider a handwritten digits classification problem in which each handwritten digit is stored in a 28x28 grayscale image. The goal is to classify the images into one of the ten digits (0 to 9). In this example, the input layer size is 784 neurons, one for each pixel in the image. The output layer size is 10 neurons, one for each class. The sizes of the hidden layers are defined arbitrarily and should be controlled to optimize the performance (e.g., classification accuracy) of the network.

In \textit{multiclass} problems, such as the handwritten digit classification, the \textit{softmax} activation function is used in the output layer to create a probability distribution over the classes. Thus, the NN predicts the class with maximum probability. In \textit{binary} problems, the \textit{logistic} activation function is typically used to map the output layer into the probability that the label is one $P(y = 1)$. Thus, the NN predicts $1$ if $P(y = 1) > 0.5$ and $0$ otherwise. The activation functions in the hidden layers are decided arbitrarily, and they also affect the performance of the NNs. Three of the most common activation functions used in the hidden layers are: \textit{logistic}, \textit{tanh}, and \textit{ReLu}. Table \ref{tab:activation} defines each of these functions as well as the softmax function.

\begin{table}[h]
    \centering
    % \setlength{\tabcolsep}{4pt}
    \begin{tabular}{lll}
        \textbf{Name} & \textbf{Function} \\
        Logistic (sigmoid) & $\sigma(x) = \frac{1}{1 + e^{-x}}$ \\
        Hyperbolic tangent (tanh) & $tanh(x) = \frac{e^x - e^{-x}}{e^x + e^-x}$ \\
        Rectified linear unit (ReLu) & $relu(x) = max(0, x)$ \\
        Softmax & $softmax(x) = \frac{e^{x_i}}{\sum_{j=1}^{J} e^{x_j}}$ \\
    \end{tabular}
    \caption{A list of common activation functions used in neural networks.}
    \label{tab:activation}
\end{table}

NNs are typically trained with some variation of the \textit{gradient descent} (GD) algorithm, which optimizes all weights $W$ and $b$ of a network $N$ to minimize a given \textit{loss function} $J(W,b)$. The loss function depends on the task being modeled by $N$. However, in supervised learning, one of the most common losses is the \textit{cross-entropy}, which measures the difference between the training data distribution and the distribution modeled by $N$. Equation \ref{eq:cross_entropy} formally defines the cross-entropy loss for a single example, where $y_c$ is the target label for the class $c$, $\hat{y}_c$ is the output predicted by $N_{W,b}$ for class $c$, and $C$ is the number of classes in the prediction task.
\begin{equation} \label{eq:cross_entropy}
\begin{split}
    J(W,b) = -\sum_{c=1}^{C} y_clog(\hat{y}_c)
\end{split}
\end{equation}

% In its basic formulation, GD uses the average loss of the entire dataset to update the weights $W^i$ and $b$ of the NN $N_{W,b}$.
As shown in Algorithm \ref{alg:grad_desc}, GD works by iteratively taking steps in the opposite direction of the gradient of the loss function $J(W,b)$ with respect to all weights. For a given number of iterations called \textit{epochs}, GD (line \ref{line:grad}) computes the gradient of $J(W,b)$ for the entire training dataset and (line \ref{line:update}) updates all weights $W$ and $b$ in the opposite direction of the gradient. The \textit{learning rate} $\alpha$ is a parameter that controls the size of the training step. Computing the gradient (line \ref{line:grad}) requires calculating the partial derivatives of the loss function with respect to all weights in $N$. This calculation is typically performed by an algorithm called \textit{backpropagation}, which uses the chain rule to compute the gradient one layer at a time, iterating backwards from the output layer to avoid redundant calculations of intermediate terms in the chain rule.

\begin{algorithm}[t]
\caption{Gradient Descent}
\label{alg:grad_desc}
\begin{algorithmic}[1]
\REQUIRE Dataset $(X, Y)$, a loss function $J(W,b)$, a NN $N_{W,b}$ with
parameters $W$ and $b$, the number of epochs $e$ and the learning rate $\alpha$.
\ENSURE Updated parameters $W$ and $b$ that minimize the loss function $J(W,b)$.
\FOR{$i \gets 1$ \TO $e$ }
       \STATE $\partial W \gets \frac{\partial J}{\partial W}$, $\partial b \gets \frac{\partial J}{\partial b}$ \label{line:grad}
       \STATE $W \gets W - \alpha \, \partial W$, $b \gets b - \alpha \, \partial b$ \label{line:update}
\ENDFOR
\end{algorithmic}
\end{algorithm}

Calculating the gradients for the whole dataset to perform just one update can be very slow or intractable for datasets that do not fit in memory. \textit{Stochastic Gradient Descent} (SGD) is a variation of GD that solves this problem by splitting the training data into sets, called \textit{batches}, and performing a training step for each batch. Although SGD supports training with very large datasets, it introduces convergence issues due to the variance in the frequent updates that cause the value of the loss function to fluctuate. \textit{Adaptive Moment Estimation} (Adam) is a recent variation of SGD that mitigates this problem by having a learning rate per weight and separately adapting them during training \cite{adam14}. In practice, most practitioners use the Adam optimizer, given that successful NNs typically require large datasets that do not fit in memory\footnote{The term \textit{big data} is typically used to refer to these very large datasets.}.

\subsection{Recurrent Neural Networks}
\label{sec:rnns}

\textit{Recurrent Neural Networks} (RNNs) are an important architecture for AMC because they were specifically designed to model sequential data. RNNs process sequences $x = \{x_1, x_2, \cdots, x_t\}$ step-by-step by keeping an internal state $h_t$ that is updated every step. Each element $x_i$ is a token (e.g., words in English or pitch classes in western music) traditionally encoded as a one-hot vector. Figure \ref{fig:rnn} shows an abstract diagram of a RNN. On the left-hand side, the RNN is shown with an input layer that passes a token $x_t$ to a hidden layer $A$ that updates $h_t$. A loop in the hidden layer allows information to be passed from one step of the network to the next. The right-hand side shows an unrolled version of the same RNN. The output layer of the network is omitted from the diagram because RNNs can produce one output per time step or one single output at the very last time step. This configuration depends on the learning task. However, the output layer typically maps the hidden state $h_t$ to an output vector $y_t$.

\begin{figure}[!h]
\centering
\vspace{0.5cm}
\includegraphics[width=0.9\columnwidth]{imgs/background/rnn.png}
\caption{Diagram of a RNN \cite{olah2015understanding}.}
\label{fig:rnn}
\end{figure}

Two of the most simple RNNs are the \textit{Elman} network \cite{elman1990finding} and the \textit{Jordan} network \cite{jordan1986}. They are composed of an input layer, a single hidden layer, and an output layer. As shown in Equation \ref{eq:elman}, these RNNs produce an output $y_t$ for each time step $t$. The matrices $W_{xh}$ and $W_{hh}$ represent the weights of the hidden layer, and the matrix $W_{hy}$ represents the weights of the output layer. The functions $f_h$ and $f_y$ are the activation functions of the hidden and output layers, respectively. The only difference between these two networks is that, in the Elman network, the weights $W_{hh}$ are fed from the hidden layer and, in the Jordan network, from the output layer.
\begin{align}
\label{eq:elman}
\begin{split}
    &\textbf{Elman network} \\
    h_t &= f_h(W_{xh}x_t + W_{hh}h_{t-1}) \\
    y_t &= f_y(W_{hy}h_t)
\end{split}
\begin{split}
    &\textbf{Jordan network} \\
    h_t &= f_h(W_{xh}x_t + W_{hh}y_{t-1}) \\
    y_t &= f_y(W_{hy}h_t)
\end{split}
\end{align}

\citet{todd1989connectionist} presented one of the first applications of RNNs for AMC: a \textit{Jordan} network designed to generate melodies. The input of this network is a melody encoded as a sequence of pitch classes (e.g., \textit{CDEGFEDF}), and the output is a single pitch. This network was trained to reconstruct given example melodies. Each melody contributed to $t$ training steps, where $t$ is the size (number of pitches) of the melody. At every training step $t$, the current example melody is given as input, and the network error is calculated by comparing the network output $y_t$ with the respective pitch $x_t$ of the input melody. After training, one can give new melodies as input to the network, which outputs new pitches by interpolating between the melodies seen during training. \citet{duff1989backpropagation} presented another early example of Jordan network for melody generation. However, instead of encoding melodies as a sequence of pitch classes, \citet{duff1989backpropagation} encoded them as a sequence of note intervals\footnote{The distance in pitch between two notes.}.

One of the major problems of RNNs consists of modeling long-term dependencies between symbols in a sequence. Modeling long-term dependency consists of creating a RNN capable of considering previous symbols that are distant from the one that is being predicted. In practice, simple RNNs are unable to connect the information between symbols that are very far from each other \cite{bengio1994}. In music, modeling long-term dependencies is critical to generate long complete pieces with coherent form.

\subsection{Long Short-Term Memory Networks}
\label{sec:lstm}
% LSTM and GRU

Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long} are a special type of RNN explicitly designed to solve the long-term dependency problem. LSTMs also process sequences $x = \{x_1, x_2, \cdots, x_t\}$ step-by-step by keeping an internal state $h_t$ that is updated every step. However, as shown in Figure \ref{fig:lstm}, the hidden layers $A$ have a different structure allowing LSTMs to capture longer dependencies in the input sequences.
A single LSTM module $t$ is composed of an extra state $C_t$ called \textit{cell state}, which is responsible for carrying information through the entire LSTM network.

\begin{figure}[!h]
\centering
% \vspace{0.5cm}
\includegraphics[width=0.9\columnwidth]{imgs/background/lstm.png}
\caption{Diagram of an LSTM \cite{olah2015understanding}.}
\label{fig:lstm}
\end{figure}

The flow of information in the cell state is controlled by three \textit{gates}: an input gate $i_t$, an output gate $o_t$ and a forget gate $f_t$. These gates facilitate the cell to remember or forget information for an arbitrary amount of time. Equation \ref{eq:lstm} formally defines the computation performed in each LSTM module, where $W_f$, $W_i$, $W_c$, and $W_o$ are weight matrices, $[h_{t-1}, x_t]$ is the the hidden state vector $h_{t-1}$ concatenated with the input vector $x_t$, and $\tilde{C}_t$ is the candidate vector to be added to the cell state. Each gate $i_t$, $o_t$, and $f_t$ have the exact same equation, just with different weight matrices ($W_i$, $W_o$ and $W_f$, respectively). The cell state $C_t$ combines the input and forget gates to control the amount of information that will be included from the input versus the amount of information that will be forgotten from the current cell state, respectively. The output gate controls the parts of the cell state that will be included in the final hidden state $h_t$. Modern RNNs (including LSTMs) typically have an extra layer, called \textit{embedding} layer, that is added before any other hidden layer to transform the one-hot input vectors $x_t$ into a dense vector representation called \textit{embeddings}. In NLP, a word \textit{embedding} is a learned representation for text where words that have the same meaning have a similar representation.
% The \textit{embedding} layer learns a dense vector representation of arbitrary dimension from the sparse one-hot representation of the symbols in the vocabulary.
\begin{equation} \label{eq:lstm}
\begin{split}
    f_t &= \sigma(W_f[h_{t-1}, x_t]) \\
    i_t &= \sigma(W_i[h_{t-1}, x_t]) \\
    \tilde{C}_t &= tanh(W_c[h_{t-1},x_t]) \\
    C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \\
    o_t &= \sigma(W_o[h_{t-1}, x_t]) \\
    h_t &= o_t * tanh(C_t)
\end{split}
\end{equation}

% TODO: Add figure with embedding example
% TODO: bidirectional LSTMs

Modern LSTM-based AMC systems typically train an LSTM as a LM, i.e. to predict the next token $x_{t+1}$ given the current token $x_t$ from training music pieces $x = \{x_1, x_2, \cdots, x_n\}$. Thus, the input token $x_t$ at each time step $t$ is mapped, with a \textit{softmax} activation function, to a probability distribution $\hat{y}_t$ over the symbols defined in the music vocabulary. The LSTM is then trained with the cross-entropy loss function, which compares the predicted probability distribution $\hat{y}_t$ with the true next token $x_{t+1}$. For example, BachBot \cite{liang2017automatic} uses an LSTM to generate polyphonic music in the style of Bach's chorales. BachBot was trained with the JSB Chorales dataset \cite{boulanger2012modeling}, where each chorale was encoded with a sequence of sixteenth-note frames. Each frame consists of four tuples (soprano, alto, tenor, and bass) in the form (\textit{pitch}, \textit{tie}), where \textit{pitch} represents a MIDI pitch number, and \textit{tie} is a boolean value that distinguishes whether a note is tied with another note at the same pitch from the previous frame or is articulated at the current time step. DeepBach \cite{hadjeres2017deepbach} is similar system that use LSTMs to geneate Bach chorales. The main difference between DeepBach and BachBot is that, in DeepBach, LSTMs consider both past and future contexts to predict the next token, while BachBot considers only the past.

% \citet{lyu2015modelling} combined an LSTM with another type of neural network called Restricted Boltzman Machine (RBM) to generate polyphonic music with different musical styles. They experimented with both the Musedata and the JSB Chorales \cite{boulanger2012modeling}. The pieces were encoded as a sequence of pitch numbers temporally aligned on an integer fraction of a quarter note. This encoding system only considered pitch and duration and hence did not considered dynamics information (e.g., note velocity).

\citet{Mao_2018} built upon a biaxial LSTM \cite{johnson2017generating} to create a system called DeepJ, which can compose music conditioned on a specific mixture of composer styles. DeepJ uses a piano roll representation augmented with dynamics information, where the style of the music piece is encoded as a one-hot representation over all artists in the training data. \citet{oore2017learning} proposed another LSTM that can generate music with dynamics. They trained an LSTM on the piano pieces from the International e-Piano Competition \cite{yamahaEPiano} with a new encoding method that extracts tempo and velocity information from MIDI messages.

\subsection{Transformers}

\textit{Transformers} \cite{vaswani2017attention} are modern architectures for sequence modeling based on \textit{attention} mechanisms. In neural NLP models, an attention mechanism is a part of a NN that dynamically highlights relevant tokens of the input sequence \cite{bahdanau2014neural}. Instead of keeping an internal hidden state that is updated at each time step like RNNs, transformers process entire sequences at once, associating an \textit{attention} score to each input token, which determines how much that token contributes to the output. Because transformers process tokens in parallel, they can take advantage of the parallel computing offered by GPUs, and hence transformers can be trained considerably faster than LSTMs \cite{vaswani2017attention}. One drawback of transformers is that they can only process sentences with a fixed size instead of LSTMs that can process sentences of any size.

Transformers were originally designed in the context of \textit{machine translation}, an NLP task that consists of translating a sequence from one language (e.g., English) to another (e.g., French). Machine translation is a \textit{sequence-to-sequence} problem, where a sequence input $x$ has to mapped into an output sequence $y$. NNs designed for machine translation normally have a \textit{encoder-decoder} structure. The first part of the network, called the \textit{encoder}, takes a sequence as input $x$ and outputs a vector representation $e$ (called \textit{encodings}) of the input $x$. The second part, called \textit{decoder}, takes the encodings $e$ as input and outputs a sequence $y$.

As shown in Figure \ref{fig:transformer}, the transformer has an encoder-decoder structure (the encoder is showed on the left side and the decoder on the right side). The transformer takes as input a sentence typically encoded with one-hot vectors and transforms it into two sequences: a sequence of \textit{input embeddings} and a sequence of \textit{positional encodings}. The former is a dense vector representation of words learned from the sparse one-hot input. The latter is a dense vector representation of the words' positions learned from the indices of the words in the sentence. The transformer adds the input embeddings and positional encodings together and passes the result through the encoder.

% TODO: Add figure with input + positional embedding example

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.55\columnwidth]{imgs/background/transformer.png}
 \caption{Diagram of a transformer \cite{vaswani2017attention}.}
 \label{fig:transformer}
\end{figure}

The encoder converts the (input + position) embeddings $e$ into \textit{encodings} with a stack of $n$ identical layers called \textit{transformer blocks}. Each transformer block has two layers: a \textit{multi-head attention layer} and a fully connected \textit{feedforward layer}. A residual connection \cite{he2016deep} is applied around each of the two layers, followed by a layer normalization \cite{ba2016layer}. A residual connection is a connection between non-contiguous layers. Layer normalization normalizes the activations of the previous layer, i.e. it applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.

The key component of the transformer is the \textit{multi-head attention layer}, which computes a score matrix $Z$ from the embeddings $e$. The scores in $Z$ represent the relationship between different words in the input sentence. For example, consider the sentence ``The animal didn't cross the street because it was too tired.''. In this sentence, the word ``it'' is related to ``animal'', and so when the transformer is processing the word ``it'', self-attention allows it to associate ``animal'' with ``it''. Figure \ref{fig:attn} shows the encoder self-attention distribution for the word ``it'' in this example.

\begin{table}[!h]
    \centering
    \includegraphics[width=0.6\columnwidth]{imgs/background/attn.png}
    \caption{The encoder self-attention distribution for the word ``it'' in the sentence
    ``The animal didn't cross the street because it was too tired.'' \cite{vaswani2017attention}.}
    \label{fig:attn}
\end{table}

The score matrix $Z$ is computed similarly to a dictionary lookup: it takes a \textit{query} matrix $Q$, a \textit{key} matrix $K$, and a \textit{value} matrix $V$, and outputs a weighted sum of the values that correspond to the keys that are most similar to the query. One of the most common self-attention mechanisms in a transformer is the scaled dot-product attention, which is shown in Equation \ref{eq:attn}. The matrices $Q$, $K$, and $V$ are created by packing the embeddings $e$ of all the words in the input sentence into a matrix $E$, and multiplying it by the weight matrices $W_q$, $W_k$ and $W_v$ that are learned during training.
\begin{equation}\label{eq:attn}
\begin{split}
    Z = Attention(Q, K, V) &= softmax(\frac{QK^T}{\sqrt{d_k}})V \\
    Q &= E * W_q \\
    K &= E * W_k \\
    V &= E * W_v
\end{split}
\end{equation}

The transformer decoder is similar to the encoder, but it has an extra \textit{masked multi-head attention} layer prepended to the transformer block to perform attention over the target output sentence $y$. This extra layer uses a mask to ensure that the predictions for position $i$ depend only on the known outputs at positions less than $i$. The output of the decoder is computed with a linear layer followed by a softmax activation.

One can use the transformer decoder (without the encoder) to train a LM capable of generating sequences (e.g., text or music) similarly to an LSTM LM (see Section \ref{sec:lstm}). Equation \ref{eq:transformer} formally defines a decoder-based transformer LM, where $x = \{x_1, x_2, \cdots, x_t\}$ is the input sequence, $n$ is the number of transformer blocks (hidden layers), $W_i$ is input embedding weight matrix, and $W_p$ is the positional encoding weight matrix.
\begin{equation}\label{eq:transformer}
\begin{split}
    h_0 &= W_ix + W_p \\
    h_l &= transformer\_block(h_{l-1}) \forall i \in [1,n] \\
    P(u) &= softmax(h_nW_e^T)
\end{split}
\end{equation}

Transformers are currently the state-of-the-art of both natural and music language modeling. For example, Radford et. al. \cite{Radford2018, radford2019language} proposed a series of models called GPT (General Pre-trained Transformer), GPT-2, and GPT-3 that used the transformer decoder to create a model of natural language. Besides generating long coherent sequences of text, pre-trained GPT models can be fine-tuned to perform specific NLP tasks (e.g., commonsense reasoning, question answering, and summarization) with state-of-the-art performance \cite{radford2019language}. Pre-training consists of training the GPT model as a (unsupervised) LM with a huge general dataset (e.g., Wikipedia). Fine-tuning is performed by stacking extra layers onto the pre-trained model and training these layers with a smaller (supervised) labeled dataset explicitly created for the task.

Music Transformer \cite{huang2018music} is one of the first transformer-based LM designed for AMC. It uses a new relative attention mechanism that improves memory consumption of the original decoder, allowing it to process longer sequences. Music Transformer achieved state-of-the-art performance on the MAESTRO dataset \cite{hawthorne2018enabling}. \citet{donahue2019lakhnes} showed that a transformer can also compose multi-instrument scores by training it with the NES MDB \cite{donahue2018nesmdb} dataset. \citet{donahue2018nesmdb} used a transfer learning procedure similarly to \citet{Radford2018}. \citet{donahue2018nesmdb} first pre-trained the transformer with the Lakh dataset (multiple instruments) and then fine-tuned it with the NES-MDB (four instruments). They manually defined a mapping between the instruments from the two datasets. Pop Music Transformer \cite{huang2020pop} is a transformer model with a specialized music representation to compose pop piano music. It was shown to generate a better rhythmic structure than previous transformer models.

% TODO: Convolutional Neural Networks

\subsection{Variational Autoencoders}

\textit{Variational autoencoders} (VAEs) \cite{kingma2013auto} are another modern architecture that can be used to generate music. VAEs are different from RNNs and Transformers because they were not specifically designed to model sequences. Instead, they are generative models that can potentially learn to represent data in any domain. VAEs have an architecture similar to a traditional \textit{autoencoder}, which is an encoder-decoder NN used to learn efficient encodings of unlabeled data (unsupervised learning). Figure \ref{fig:ae} shows a diagram of an autoencoder.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.7\columnwidth]{imgs/background/autoencoder.png}
 \caption{Diagram of an autoencoder \cite{rocca2019understanding}.}
 \label{fig:ae}
\end{figure}

An autoencoder builds a latent space of a dataset $X$ with an encoder network $e$ by learning to compress each example $x$ into a vector $z$ and then reproducing $x$ from $z$ with a decoder network $d$. A key component of an autoencoder is the bottleneck introduced by making the vector $z$ have fewer dimensions than the input $x$, which forces the model to learn a compression scheme. During training, the autoencoder ideally distills the qualities that are common throughout the dataset. As shown in Figure \ref{fig:ae_sampling}, one can use an autoencoder as a generative model by sampling random vectors from the latent space learned by the encoder and using the trained decoder to build the output from the sampled vector.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.7\columnwidth]{imgs/background/ae_sampling.png}
 \caption{Sampling from the latent space built by an autoencoder \cite{rocca2019understanding}.}
 \label{fig:ae_sampling}
\end{figure}

One limitation of the autoencoder is that it often learns a latent space that is not continuous\footnote{Two close points in the latent space should yield similar outputs when decoded.} nor complete\footnote{Any point sampled from the latent space should yield a ``meaningful'' output when decoded} \cite{roberts2018hierarchical}. This means that if one decodes a random vector sampled from the learned latent space, it might not result in a realistic output. VAEs solve this problem by encoding the dataset $X$ as a probability distribution over the latent space instead of a single vector $z$. Typically, this distribution is assumed to be a multivariate normal distribution $N(\mu_x, \sigma_x)$. Figure \ref{fig:vae} shows a diagram of a VAE. The difference between an autoencoder and a VAE is that the VAE encoder outputs two vectors $\mu_x$ and $\sigma_x$, instead of a single vector $z$. These two vectors represent the mean and standard deviation of a normal distribution $N$, respectively. The decoder $d$ samples a vector $z \sim N(\mu_x, \sigma_x)$ from the distribution $N$ and reconstructs the input $x$ from $z$.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.8\columnwidth]{imgs/background/vae.png}
 \caption{Diagram of a variational autoencoder \cite{rocca2019understanding}.}
 \label{fig:vae}
\end{figure}

MusicVAE \cite{Roberts2017} is one of the first examples of VAE applied to AMC. It splits the input sequence $x$ into $U$ non-overlapping subsequences $y_u$, such that $x = \{y_1, y_2, \cdots, y_U\}$. The encoder process the segmented input $x$ with a bidirectional LSTM, whose hidden states are used to produce the latent distribution parameters $\mu_x$ and $\sigma_x$. The decoder is a novel hierarchical LSTM that takes the latent vector $z \sim N(\mu_x, \sigma_x)$ as input and first produces $U$ embedding vectors $e = \{e_1, e_2, \cdots, e_U\}$ (one for each subsequence $y_u$) with an LSTM called \textit{conductor}. Each embedding vector is then passed through a  fully connected layer to produce the initial states for a final decoder LSTM. The final LSTM then autoregressively produces a sequence of distributions over output tokens for each subsequence $y_u$ via a softmax activation.

MIDI-VAE \cite{brunner2018midi} uses a VAE to perform style transfer in polyphonic symbolic music. It works by separating a portion $z_s$ of the latent vector $z$ for style classification and another portion $z_t$ to encode music structure (pitch, timber, and velocity). During generation, one can change the style $s_i$ of a given piece $x$ to another style $s_j$ by passing $x$ through the encoder to get $z$ and swapping the values $z_s^i$ and $z_s^j$. The modified latent vector is then passed through the decoder to get $x$ with the target style $s_j$. Style labels can be music genres such as Jazz, Pop, and Classic; or composer names such as Bach or Mozart.
VirtuosoNet \cite{jeong2019virtuosonet} is a VAE designed to generate piano performances with expressive control of tempo, dynamics, and articulations. The encoder is a hierarchical LSTM that encodes music on different levels: note, beat, and measure. The decoder renders musical expressions by first predicting the tempo and dynamics in measure level and then refining the result in note level.

\subsection{Generative Adversarial Networks}

\textit{Generative adversarial networks} (GANs) \cite{goodfellow2014generative} are another recent class of generative models that, in theory, can generate synthetic data in different domains. GANs are composed of two independent NNs: a \textit{generator} and a \textit{discriminator}. In its most basic form, the generator takes random noise as input and transforms it into a fake example. The discriminator is a binary classifier that discriminates examples as \textit{fake} (0) or \textit{real} (1). The generator and discriminator architectures depend on the generative task that one wants to perform with the GAN. For example, Figure \ref{fig:gan} illustrates a GAN for handwritten digit generation. The random noise is a matrix $M$ representing a grayscale image, which can be generated by sampling values between 0 (black) and 1 (white) from a uniform distribution. Typically, the generator uses convolutional layers to transform $M$ into a fake handwritten digit. The discriminator then combines real and fake images to learn how to separate images between fake and real.

\begin{figure}[!h]
 \centering
 \includegraphics[width=\columnwidth]{imgs/background/gan.png}
 \caption{Diagram of a generative adversarial network \cite{silva2018intuitive}.}
 \label{fig:gan}
\end{figure}

Training a GAN consists of training the generator and the discriminator together
iteratively in alternating periods:

\begin{enumerate}
    \item \textbf{The discriminator trains for one or more epochs.}

    The discriminator is trained with a loss function that penalizes it for misclassifying a real instance as fake or a fake instance as real. The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.

    \item \textbf{The generator trains for one or more epochs.}

    The generator is trained with a loss function that penalizes it for producing a sample that the discriminator  classifies as fake. In other words, the generator loss is computed via the discriminator. Thus, the generator updates its weights through backpropagation from the discriminator to the generator.
\end{enumerate}

The original GAN uses a single loss function called \textit{minimax loss} to train both the generator and the discriminator \cite{goodfellow2014generative}. Minimax loss is derived from the cross-entropy between the real and generated distributions. The generator tries to minimize this loss while the discriminator tries to maximize it.
% Equation \ref{eq:minimax_loss} formally defines the minimax loss, where $D(x)$ is the discriminator's estimate of the probability that real data instance x is real, $E_x$ is the expected value over all real data instances, $G(z)$ is the generator's output when given noise z, $D(G(z))$ is the discriminator's estimate of the probability that a fake instance is real, and $E_z$ is the expected value over all generated fake instances $G(z)$.
%
% \begin{equation} \label{eq:minimax_loss}
%     E_x[log(D(x))] + E_z[log(1 - D(G(z)))]
% \end{equation}
During training, $G$ learns to generate fake data that resembles the original data, and $D$ learns to distinguish the generator's fake data from real data. The training process aims to have a generator $G$ that produces output that can fool the discriminator $D$. After training, one can generate new data points by sampling from different distribution points learned by $G$.

GANs work well to generate continuous data such as images \cite{brock2018large}, but it has limitations in generating categorical data, especially in a sequential form such as text or music. Images can be represented by a continuous matrix $M$ and so applying transformations (e.g., $M' = M + 0.05$) to $M$ still results in defined images $M'$ that can be classified as real or fake. In sequential domains such as text and music, tokens are encoded using embedding vectors. Applying transformation to an embedding vector does not necessarily generate a valid token. For example, assume that the word ``university'' is represented by the vector $v = [0.44, 0.37, -0.28]$. If one applies the transformation $v' = v + 0.05$ to the original $v$, the new vector $v'$ not necessarily represent some word in the vocabulary. Therefore, updating the weights of $G$ with the gradients of the minimax loss might lead $G$ to generate invalid data. Another problem is that the discriminator can only provide feedback on entire sequences.

% TODO: Better summarize and correlate the different approaches
Different approaches have been proposed to solve these problems in the domain of symbolic music generation. For example, C-RNN-GAN \cite{mogren2016c} encodes MIDI files as a sequence of continuous note events. Each note is a tuple $(l, p, i, dt)$, where $l$ is the note length, $p$ is the pitch frequency, $i$ is note intensity, and $dt$ is the time elapsed since the last note. With this encoding scheme, C-RNN-GAN used RNNs for both the generator and the discriminator. SeqGAN \cite{yu2017seqgan} combined adversarial training and reinforcement learning to generate monophonic music with a RNN generator and a convolutional discriminator. MidiNet \cite{yang2017midinet} encodes MIDI files as a sequence of fixed-size piano rolls $M \in \{0, 1\}^{128*w}$, where $w$ is the number of time steps in each piano roll. Since each piano roll can be seen as a grayscale image, SeqGAN uses convolutional layers in the generator and the discriminator. MuseGAN \cite{dong2018musegan} also uses a piano roll encoding with a convolutional generator and discriminator to generate polyphonic music. \citet{muhamed2021symbolic} proposed a GAN where both the generator and the discriminator are transformers. The music pieces are encoded with the Music Transformer encoding scheme \cite{huang2018music}, and the Gumbel-Softmax trick \cite{jang2016categorical} is used to address the gradient problem of categorical generators.

\section{Decoding}

As discussed in the previous section, most neural generative models for AMC are based on RNNs, LSTMs, or Transformers\footnote{Including VAEs and GANs, which typically use RNNs, LSTMs, or Transformers as building blocks.}. These sequential models use a softmax activation function in the output layer to create a LM $L = P(x_t|x_1, \cdots, x_{t-2}, x_{t-1})$, where $\{x_1, \cdots, x_{t-2}, x_{t-1}\}$ is an input sequence and $x_t$ is the next token in that sequence. Typically, an \textit{autoregressive} strategy is used to generate music with $L$, i.e. to \textit{decode} the softmax output into a sequence of music tokens. One starts with a prior sequence of tokens $x = \{x_1, x_2, \cdots, x_{t 1}\}$, which is fed into $L$ to generate $L(x) = x_{t}$. Next, $x_{t}$ is concatenated with $x$ and the process repeats until a special end-of-piece token is found or a given number of tokens are generated. As defined in Equation \ref{eq:autoregressive}, autoregressive generation assumes that the probability distribution of a sequence of tokens can be decomposed into the product of conditional next token distributions.

\begin{equation} \label{eq:autoregressive}
    P(x_t|x_1, \cdots, x_{t-2}, x_{t-1}) = \prod_{t=1}^{T} P(x_t|x_{1:t-1})
\end{equation}

Currently, most prominent autoregressive strategies for music (and text) decoding are based either on sampling or searching. While sampling is well suited for creative tasks such as music composition, searching fits better generative problems where specific solutions are expected, such as machine translation.

\subsection{Top-k Sampling}

In its most basic form, sampling consists of randomly picking the next token according to the conditional probability distribution given by the LM: $x_t \sim P(x_t|x_{1:t-1})$. Figure \ref{fig:topk} shows and example of text generation with the prior sequence $x = \{\textit{The}\}$. In the first step, the word \textit{car} is sampled from the condition probability distribution $P(x_1 | \textit{The})$ and, in the second step, the word \textit{drives} is sampled from $P(x_2 | \textit{The}, \textit{car})$.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.9\columnwidth]{imgs/background/sampling.png}
 \caption{Example of sampling with prior sequence $x = \{\textit{The}\}$ \cite{platen2020}.}
 \label{fig:topk}
\end{figure}

A simple trick called \textit{temperature} can be applied to control the confidence of the LM. It consists of dividing the LM output before the softmax activation by a parameter $t > 0$. Lower temperatures $t < 1$ make the model increasingly confident in its top choices, while $t > 1$ decreases confidence. Figure \ref{fig:temp} shows the previous example with temperature $t = 0.7$. The conditional probability $P(x_1 | \textit{The})$ of the first step becomes more confident, leaving almost no chance for the word \textit{car} to be selected. Thus, the word \textit{nice} is sampled first, followed by the word \textit{house}.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.9\columnwidth]{imgs/background/temperature.png}
 \caption{Example of sampling with prior sequence $x = \{\textit{The}\}$ and temperature $t = 0.7$ \cite{platen2020}.}
 \label{fig:temp}
\end{figure}

\textit{Top-k sampling} \cite{fan2018hierarchical} is another way to control the probability distribution of the LM. It consists of using only the $k$ most likely tokens in the distribution, redistributing the probability mass among only those \textit{top-k} tokens. With this approach, the LM is filtered at each generation step and the token is picked randomly according to the resuling probability distribution. Figure \ref{fig:topk} illustrates the previous example with top-k sampling ($k=6$).

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.9\columnwidth]{imgs/background/topk.png}
 \caption{Example of top-k sampling with $k=6$.}
 \label{fig:topk}
\end{figure}

In step $t=1$, the top-k sampling keeps the six words $\{\textit{nice}, \textit{dog}, \textit{car}, \textit{woman}, \textit{guy}, \textit{man}\}$ in the sampling pool, which encompass only two thirds of the probability mass. The words $\{\textit{people}, \textit{big}, \textit{house}, \textit{cat}\}$ are eliminated, even though they seem like reasonable candidates. In step $t=2$, the top six words represent almost all of the probability mass. Two of the selected words $\{\textit{down}, \textit{a}\}$ are arguably bad candidates. Nevertheless, the eliminated words $\{\textit{not}, \textit{the}, \textit{small}, \textit{told}\}$ are rather bad candidates and hence successfully eliminated. This example highlights that top-k sampling can jeopardize the LM, making it produce wrong sentences for sharp distributions and limiting the model's creativity for flat distributions.

\subsection{Top-p (Nucleus) Sampling}

Holtzman et. al. \cite{holtzman2019curious} proposed \textit{top-p (nucleus) sampling} to address the degeneration problems faced by top-k sampling (and other decoding strategies). Instead of limiting the sample pool to a fixed size $k$, top-p samples from a dynamic \textit{nucleus}, i.e. the smallest set of tokens whose cumulative probability exceeds a given probability $p$. Thus, the size of the sample pool is dynamically adjusted for each step depending on the LM distribution. Figure \ref{fig:topp} shows the previous example with top-p sampling ($p=0.92$).

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.9\columnwidth]{imgs/background/topp.png}
 \caption{Example of top-p sampling with $p=0.92$ \cite{platen2020}.}
 \label{fig:topp}
\end{figure}

The nucleus for $p=92\%$ includes the nine most likely words in the first step and only three words in the second step. This example highlights that top-p sampling keeps a wide range of tokens in less predictable situations (e.g., step $t=1$) and a few tokens in more predictable situations (e.g., step $t=2$). Although top-p is theoretically more interesting than top-k, both methods work well in practice. Top-p can also be combined with top-k to avoid very low ranked tokens while allowing for some dynamic selection.

\subsection{Greedy Search}

Greedy search selects the token with the highest probability at each generation step $t$: $x_t = argmax \, P(x_t|x_{1:t-1})$. Figure \ref{fig:greedy} shows a text generation example with greedy decoding and prior sequence $x = \{\textit{The}\}$.
In step $t=1$, greedy search selects the word $\textit{nice}$, which has the highest probability among the three options $\{ \textit{dog}, \textit{nice}, \textit{car} \}$. In step $t=2$, the options are $\{ \textit{woman}, \textit{house}, \textit{guy} \}$ and hence the greedy search selects the word \textit{woman}. The final generated sentence is \textit{The nice woman} with a joint probability of $0.5*0.4=0.2$.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.7\columnwidth]{imgs/background/greedy.png}
 \caption{Example of greedy search with prior sequence $x = \{\textit{The}\}$ \cite{platen2020}.}
 \label{fig:greedy}
\end{figure}

The problem with greedy search is that it misses high probability tokens ``hidden behind'' low probability ones. In this example, the global optimal solution is the sentence \textit{The dog has} (with joint probability $0.36$), but the word \textit{nice} has higher probability than \textit{dog} in step $t=1$. Thus, greedy search selects \textit{nice} and completely disregards the branch with the word \textit{dog} in step $t=2$.

\subsection{Beam Search}

\textit{Beam search} reduces the risk of missing hidden high probability tokens by keeping the most likely $b$ solutions (called \textit{beams}) at each time step, where $b$ is a parameter called \textit{beam width}. At the final generation step, the solution (or beam) with the highest joint probability is selected. Figure \ref{fig:beam} shows how beam search is capable of finding the best solution of the previous example with beam size $b=2$.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.7\columnwidth]{imgs/background/beam.png}
 \caption{Example of beam search with prior sequence $x = \{\textit{The}\}$ and
 beam size $b=2$ \cite{platen2020}.}
 \label{fig:beam}
\end{figure}

In step $t=1$, the two most likely sub-sequences are $\{ \textit{The}, \textit{dog} \}$ and $\{ \textit{The}, \textit{nice} \}$. In step $t=2$, beam search expands the beam from the previous step with the two most likely sub-sequences $\{ \textit{The}, \textit{dog}, \textit{has} \}$ and $\{ \textit{The}, \textit{nice}, \textit{woman} \}$. At the end of the second step, beam search returns the beam with highest probability, which is \textit{The dog has} with joint probability $0.36$. The solutions generated with beam search are always as good as or better (more likely according to the LM) than the sequences generated with greedy search. However, beam search is not guaranteed to find the optimal solution.

A common problem with beam search (and other search-based approaches) is that the decoded sequences tend to become repetitive in a few generation steps \cite{holtzman2019curious}. This problem can be alleviated by combining beam search with sampling (including top-k and top-p) to create a \textit{stochastic beam search} \cite{poole2010artificial}. In this case, the sub-sequences are sampled at each time step according to their joint probability, instead of selected greedly.

Different \textit{decoding} strategies (e.g., top-k sampling and beam search) can be used to generate music with different NN \textit{architectures} (e.g., LSTM, Transformer, VAEs, and GANs) trained with different \textit{symbolic music} datasets (e.g. MAESTRO, Lakh, and JSB Chorales). This chapter presented an overview of these fundamental ideas of deep learning for music generation. This dissertation builds upon these ideas for controlling the emotion of music composed with neural LMs. The next chapter reviews previous works that are related to this dissertation, including methods to control neural LMs and other AMC systems that generate music with a target emotion.
