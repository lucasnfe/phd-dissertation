As discussed throughout this dissertation, the problem of composing music with a target emotion has been explored with different approaches, but only recently, with the rise of deep learning, NNs started to draw the attention of the AAC community.  Indeed, this dissertation is one of the first works to approach AAC as a deep learning problem. Together with a few other recent works \cite{madhok2018sentimozart,tan2020automated,zhao2019emotional, tan2020music}, the contributions of this dissertation have opened up different possibilities of research that can be explored in the future. The remainder of this chapter describes different possibilities of future work that the AAC community can explore from this dissertation.

\section{Datasets}

% Increase size
Successful supervised deep learning systems typically are trained on hundreds of millions of labeled examples \cite{lecun2015deep}. The available datasets for AAC, including the VGMIDI dataset, have only hundreds of labeled examples. One of the major contributions one could bring to support deep NNs for AAC would be to extend the VGMIDI dataset or create a new dataset with a considerably larger number of labeled examples. Labeling symbolic music according to emotion is a subjective task because people can perceive emotions differently in music (as evidenced by the VGMIDI annotation process described in Chapter \ref{ch:ismir19}). This subjectivity makes the labeling task expensive once many annotators have to be assigned to the same piece in order to compute a meaningful (and democratic) final label. Therefore, another future work is to design a service or a game \cite{Law2007TagATuneAG} where users would have an external incentive (other than money) to collaborate in the task of labeling music with a target emotion.

% Increase instruments
Most of the AAC datasets, including the VGMIDI, are limited to piano music. An important future work is to create datasets of multi-instrumental polyphonic music labeled according to emotion. These datasets would allow NNs to model the relationship between different timbres and perceived emotions. A significant challenge in creating such datasets is to collect a large collection of pieces that use the same set of instruments. Typically, the symbolic music datasets are created by gathering MIDI files publicly available on the Internet. Since these MIDI files come from different sources on the Internet, it is hard to guarantee consistency among the files. Once again, this problem could be addressed with an online service (or game), but instead of asking users to label pre-authored pieces, the system should support music composition with a given set of instruments and target emotions.
% Moreover, when designing such system, it would be important to use high quality synthesizers that approximate as close as possible to real instruments.

% Different styles of music
A specific problem with the VGMIDI dataset is that human subjects consider some of its negative pieces ambiguous. Thus, another future work consists of creating a dataset of less ambiguous pieces. This problem can be addressed by creating a dataset of movie soundtracks and asking annotators to label the pieces together with their respective scenes. The problem with building soundtrack datasets is that most of these pieces are not publicly available in symbolic format. One way to solve this problem is to ask professional composers to create scores for a set of video clips that evoke different emotions.

\section{Music Representation}

% Learn from audio
Most datasets of symbolic music are organized as MIDI files. To learn models from these datasets, one has to create a vocabulary of music tokens extracted from the MIDI events or from the piano roll representation of the MIDI files. This dissertation explored vocabularies of different sizes. However, it is still unclear how different vocabularies affect the quality and controllability of the models. This analysis is a future work that should be performed with different decoding strategies (e.g., TopK, SBBS, and MCTS) and different data regimes (low data vs. big data). For example, one can investigate the impact of different vocabulary sizes on the level of repetition in pieces generated by different decoding algorithms. Another future work is to use a \textit{vector quantized variational autoencoder} (VQ-VAE) to learn a vocabulary from MIDI files. Learning a vocabulary instead of manually defining one can yield better music representations that increase the overall quality and controllability of the models.

Although MIDI files provide easy access to high level music information (e.g., melody, harmony, and rhythm), they cannot capture human voices or many of the more subtle timbres, dynamics, and expressivity that the audio format can. The problem with modeling digital audio is that audio sequences are very long. For example, a typical four-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. The deep learning community has actively explored this problem \cite{oord2016wavenet, mehri2016samplernn, yamamoto2020parallel}, but the state-of-the-art methods still generate music that is significantly far from human composed music \cite{dhariwal2020jukebox}. Designing NNs capable of modeling music from audio signals is an important future work that can potentially increase the expressivity of neural AAC systems.

\section{Modeling}

This dissertation explored three different approaches to control emotion in music generated by LMs. The first one is a genetic algorithm that optimizes the LM weights, and the other two are decoding strategies that steer the LM's distribution at generation time. These three approaches rely on pre-trained neural LMs with architectures especially designed to process sequences (e.g., RNNs and transformers). As future work, one can investigate how to control GANs and VAEs to compose music with a target emotion. GANs have shown great potential in music generation \cite{muhamed2021symbolic}. Moreover, conditional GANs have shown a great level of controllability in image generation \cite{mirza2014conditional}. One can train a conditional GAN to generate music where the conditions are input signals representing emotions in a categorical or dimensional model of emotion. The problem with using GANs for AAC is that training GANs with relatively small datasets typically leads to discriminator overfitting, causing training to diverge \cite{karras2020training}. Regarding VAEs, one can extend Music FaderNets \cite{tan2020music} to learn a representation between low-level music features and different emotions.

Another interesting direction of future work consists of fine-tuning generative models (e.g., transformers) with reinforcement learning. This can be done by pre-training a high-capacity LM on a large unlabelled dataset of symbolic music and then using reinforcement learning to steer this LM towards different emotions. Ideally, the reinforcement learning fine-tuning should yield different policies for different emotions. Thus, after fine-tuning, the same LM could be used to compose music with different target emotions.

\section{Decoding}

SBBS and MCTS have shown good potential to control emotion in music generated by LMs. As future work, one can apply these methods to generate sequences in different domains. For example, one can use MCTS to generate text, sketches, or video game levels. Moreover, one can investigate how to improve the quality of the music generated by both these methods -- for example, exploring how to reduce repetition in the music pieces generated by SBBS. This can be done by training a repetition discriminator to model a common level of repetition in human composed music \cite{holtzman2018learning}. This discriminator should be combined with the emotion discriminator at decoding time. One can also apply trainable weights to each of these discriminators to linearly adjust their contributions to the final generated piece.

Section \ref{sec:related_nlp} showed different strategies to decode neural LMs in the natural language domain. Exploring these strategies to control emotion in generated music is another important direction of future work. For example, one can explore how to use the Plug and Play LM \cite{dathathri2019plug} to compose music with controllable emotion.

\section{Applications}

Bardo Composer is an important contribution towards generating affective music in real time for tabletop role-playing games. However, Bardo Composer was evaluated on the task of scoring videos of a D\&D campaign. Thus, a future work consists of building a system to score D\&D campaigns in real time and evaluating how the system affects the players' experiences. Another problem with Bardo Composer is that it uses two independent datasets of story and music, where each dataset has a different model of emotion \cite{padovani2017}. Mapping emotions from one model to the other yields a hybrid model that is not as meaningful as the original ones. This problem can be handled in a future work where one creates an integrated dataset of music and stories. This dataset should use music pieces that are typically used with tabletop games. Moreover, it should be annotated according to a model of emotion that is meaningful for music and D\&D stories.

One can also build upon Bardo Composer to generate soundtracks for other media. For example, one can directly apply the approach described in Chapter \ref{ch:aiide20} to score audiobooks. One can also extract sentiment signals from video games scenes using game events (e.g., powerup pickup) and apply Bardo Composer to generate video game soundtracks or sound effects. Similarly, one could extract emotion signals from movies using raw pixels and apply Bardo Composer to generate movie soundtracks. Another exciting application of Bardo Composer is to include it as part of virtual assistants (e.g., Google Assistant or Siri) to allow users to have personalized soundtracks based on the emotional tone of their conversations.
