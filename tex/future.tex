As discussed throughout this dissertation, the problem of composing music with a target perceived emotion has been explored with different approaches, but only recently, with the rise of deep learning, NNs started to draw the attention of the AAC community.  Indeed, this dissertation is one of the first works to approach AAC as a deep learning problem. Together with a few other recent works \cite{madhok2018sentimozart,tan2020automated,zhao2019emotional, tan2020music}, the contributions of this dissertation have opened up different possibilities of research that can be explored in the future. The remainder of this chapter describes different possibilities of future work that the AAC community can explore from this dissertation.

\section{Datasets}

% Increase size
Successful supervised deep learning systems typically are trained on hundreds of millions of labelled examples with which to train the machine \cite{lecun2015deep}. However, the available datasets for AAC, including the VGMIDI dataset, have only hundreds of labelled examples. Therefore, one of the major contributions one could bring to support deep NNs for AAC would be to extend the VGMIDI dataset or create a new dataset with a considerably larger number of labelled examples. Labelling symbolic music according to perceived emotion is a subjective task because people can perceive emotions differently in music (as evidenced by the VGMIDI annotation process described in Chapter \ref{ch:ismir19}). This makes it expensive to annotate symbolic music pieces according to perceived emotion because many annotators have to be assigned to the same piece in order to compute a meaningful (and democratic) final label. Therefore, another future work is to design a service or a game \cite{Law2007TagATuneAG} where users would have external incentive (other than money) to collaborate in the task of labelling music with a target emotion.

% Increase instruments
The VGMIDI dataset and most of the other AAC datasets are limited to piano music. Thus, a great new contribution would be to create datasets with polyphonic music with multiple instruments. This would allow one to use NNs to model the relationship between different timbres and perceived emotion. One of the major challenges in creating large datasets with multiple instruments is to collect a large collection of pieces that use exactly the same set of instruments. Typically, the symbolic music datasets are created by gathering MIDI files publicly available from the Internet.  Since the MIDI files come from different sources in the Internet, it is hard to guarantee consistency among the files. Once again, this could be addressed by making an online service (or game), but instead of asking users to label pre-authored pieces, the system should support music composition with a given set of instruments and target emotions.
% Moreover, when designing such system, it would be important to use high quality synthesizers that approximate as close as possible to real instruments.

% Different styles of music
A specific problem encountered with the VGMIDI dataset is that negative video game piano pieces are considered  ambiguous by human subjects. Thus, another future work would be to create a dataset of pieces that are less ambiguous. One way to address this would be to use movie soundtracks and ask annotators to label the pieces together with their respective scenes, for example. The problem with building soundtrack datasets is that most of this music is not publicly available in the symbolic format. One way to solve this problem would be to ask professional composers to create scores for a set of video clips that represent different emotions.

\section{Music Representation}

% Learn from audio
Most datasets of symbolic music are organized as MIDI files. To learn models from these files one has to create a vocabulary of music tokens extracted from the MIDI events or from the piano roll representation of that MIDI. This dissertation explored vocabularies with different sizes, however, it is still unclear how different vocabularies affect the quality and controllability of the generated music. This analysis should be performed with different decoding strategies and different data regimes (low data vs. big data). For example, one can investigate the impact of vocabulary size in the level of repetition in generated music. Vocabulary size can also impact the quality of representations learned by VAEs. Another possible future direction related to music representation is to learn representations from MIDI files. For example, one could use a \textit{vector quantised variational auto encoder} (VQ-VAE) to learn a vocabulary from MIDI files.  Having better music representation has the potential to increase the overall quality of generated music and support better model controllability, which are key components of neural AAC.

Although MIDI files provide easy access to high level music information (e.g., melody, harmony, and rhythm), they cannot capture human voices or many of the more subtle timbres, dynamics, and expressivity that the audio format can. The problem of modeling digital audio is that audio sequences are very long. For example, a typical four-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. This problem has been actively explored by the deep learning community \cite{oord2016wavenet, mehri2016samplernn, yamamoto2020parallel}, but the state-of-the-art methods still generate music that significantly far from human-composed music \cite{dhariwal2020jukebox}. Designing neural networks capable of modeling music from audio signals is an important future work that can potentially increase the expressivity of neural AAC systems.

\section{Modeling}

This dissertation explored three different approaches to control emotion in music generated by LMs. The first one consists of fine-tuning a LM with a genetic algorithm and the other two are decoding strategies that steer the LM's distribution in generation time. These three approaches rely on pre-trained neural LMs with architectures especially designed to process sequences (e.g., RNNs and transformers). As future work, one could investigate how to control GANs and VAEs to compose music with controllable emotion, or design completely new architectures for that purpose. GANs have shown great potential in music generation \cite{muhamed2021symbolic} and controllability through conditioning \cite{}. Thus, one could train a conditional GAN to generate music where the conditions are input signals representing emotions in a categorical or dimensional model of emotion. The problem to use GANs for AAC is that training them with too little data typically leads to discriminator overfitting, causing training to diverge \cite{karras2020training}. Regarding VAEs, a future work can be to extend Music FaderNets \cite{tan2020music} to learn a representation between low-level music structure and different emotions.

Another interesting direction of future work consists of fine-tuning generative models (e.g., transformer) with reinforcement learning. This could be done by pre-training a high-capacity LM on a large unlabelled dataset of symbolic music and then using reinforcement learning to steer this LM towards different emotions. Ideally, the reinforcement learning training procedure should yield different policies for different emotions. Thus, after fine-tuning, the same LM could be used to compose music with different target emotions.

\section{Decoding}

SBBS and MCTS have shown great potential to control emotion in music generated by LMs. As a future work, one could apply these methods to generate sequences in different domains. For example, one can use MCTS to generate text, sketches or video game levels. Moreover, one could investigate how to improve the quality of the music generated by both these methods. For example, exploring how to reduce repetition in the music generated by SBBS. This could be done by training a repetition discriminator that models a common level of repetition in human-composed music \cite{holtzman2018learning}. This discriminator should be combined with the emotion discriminators in decoding time. One could also apply trainable weights to each of these discriminators in order linearly adjust their contributions to the final generated music.

Section \ref{sec:related_nlp} showed different strategies to decode neural LMs in the natural language domain. Exploring these approaches to control emotion in generated music are another important direction of future works. For example, one could explore how to use the Plug and Play LM \cite{dathathri2019plug} to compose music with controllable emotion.

\section{Applications}

This dissertation showed some progress with Bardo Composer towards generating affective music in real time for tabletop roleplaying games. Bardo Composer was evaluated on the task of scoring videos of a D\&D campaign. Thus, a future work related to Bardo Composer consists of building a prototype to score campaigns in real time and evaluating how the system affects the players' experiences. Another problem with Bardo Composer is that it uses two independent datasets of story and music where each dataset has a different model of emotion \cite{padovani2017}. Mapping the emotions from one model to the other yields a hybrid model that is not as meaningful as the original ones. This problem can be handled in a future work where one creates an integrated dataset of music and stories. This dataset should use music pieces that are typically used with tabletop games. Moreover, it should be annotated according to a model of emotion that is meaningful for music and D\&D stories.

One could also build upon Bardo Composer to generate soundtracks for other media. For example, Bardo Composer could be applied to score audio books with the same approach used to score tabletop roleplaying games. One can extract the sentiment or emotion from video games scenes based on game events (e.g., powerup pickup) and use Composer to generate video game soundtracks or sound effects. Similarly, one could extract emotion signals from movies using raw pixels and apply Bardo Composer to generate soundtracks for movies. Another interesting application of Bardo Composer is to include it as part of virtual assistants (e.g., Google Assistant or Siri) to allow users to have personalized soundtracks based on the users conversations.
